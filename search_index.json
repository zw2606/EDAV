[
["index.html", "edav.info/ Welcome 0.1 Everything you need for Exploratory Data Analysis &amp; Visualization 0.2 Contact 0.3 License 0.4 Colophon", " edav.info/ Zach Bogart, Joyce Robbins 2019-10-07 Welcome 0.1 Everything you need for Exploratory Data Analysis &amp; Visualization This resource has everything you need and more to be successful with R, this EDAV course, and beyond. Let’s get started! With this resource, we try to give you a curated collection of tools and references that will make it easier to learn how to work with data in R. In addition, we include sections on basic chart types/tools so you can learn by doing. There are also several walkthroughs where we work with data and discuss problems as well as some tips/tricks that will help you. We hope this resource serves you well! This resource is specifically tailored to the GR5702 Exploratory Data Analysis and Visualization course offered at Columbia University. However, anyone interested in working with data in R will benefit from perusing these pages. Happy coding! 0.2 Contact Zach Bogart: Website / Twitter / GitHub Joyce Robbins: Columbia Profile / GitHub 0.3 License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. 0.4 Colophon The EDAV Logo, the url/404 banners, and associated chapter icon designs are designed by Zach Bogart and published with permission. The url and 404 banners have been adapted into a typeface called Koji. Selected chapter icons can be accessed/purchased at The Noun Project. Please attribute the creator if using them for external purposes (see their icon attribution guidelines for more information). "],
["intro.html", "1 Introduction 1.1 Overview 1.2 Types of Assistance 1.3 Help improve edav.info/ 1.4 Fun stuff 1.5 Acknowledgments", " 1 Introduction 1.1 Overview This chapter introduces how this resource is organized, explains how you can add to this resource, and includes some general acknowledgments. 1.2 Types of Assistance Chapters in this resources are color-coded to indicate the type of assistance the chapter provides. Below is an explanation of each type: 1.2.1 Information (Blue) Blue pages contain basic information. Examples of blue pages include this introduction page and the basics page, which explains how to setup R/RStudio as well as ways to get help if you need it. Blue pages are the help desk of this resource: look to them if you are lost and need to find your way. 1.2.2 Walkthroughs (Red) Red pages contain more extensive walkthroughs. An example of a red page is the iris walkthrough, where a well-known dataset is presented as a pretty scatterplot and steps are shown from start to finish. This page type is the most thorough: it tries to provide full documentation, explanations of design choices, and advice on best practices. It’s like going to office hours and having a great clarifying chat with a course assistant…in article form. If you would like to see a fully-worked-through example of something with a lot of guidance along the way, check out the red pages. 1.2.3 Documentation (Green) Green pages contain more compact documentation. An example of a green page is the histogram page, which includes simple examples of how to create histograms, when to use them, and things to be aware of/watch out for. The green pages hold your hand much less than the red pages: they explain how to use a chart/tool using examples and simple terms. If you have an idea in mind and are just wondering how to execute it, the green pages will help fill in those gaps. 1.2.4 References (Yellow) Yellow pages contain simple collections of references. An example of a yellow page is the external resources page, which is a list of materials that you can look through and learn from. Yellow pages have the least amount of hand-holding: they are collections of resources and bare-boned tutorials that will help you learn about new things. 1.3 Help improve edav.info/ This resource is an ongoing creation made by students, for students. We welcome you to help make it better. Not finding what you are looking for? Think a section could be made clearer? Consider helping improve edav.info/ by submitting a pull request to the github page. Don’t understand that last sentence? We have a page on how you can contribute to edav.info/. 1.4 Fun stuff 1.4.1 T-shirts Zach Bogart has made a few t-shirts available on Teespring so you can show your love for EDAV and R. Hope you enjoy! 1.5 Acknowledgments 1.5.1 Our Contributors Thank you so much to everyone who has contributed. You make edav.info/ possible. Aashna Kanuga (@aashnakanuga), @Akanksha1Raj, Akhil Punia (@AkhilPunia), Akshata Patel (@akshatapatel), Angela Li (@angela-li), @anipin, @AshwinJay101, Eric Boxer (@Ecboxer), @excited-student, @hao871563506, Harin Sanghirun (@harin), @jw2531, @kiransaini, @leahparreztnik, Louis Massera (@louismassera), @naotominakawa, Neha Saraf (@nehasaraf1994), Oleh Dubno (@odubno), Ramy Jaber (@ramyij), Rod Bogart (@rodbogart), @Somendratripathi, Tim Kartawijaya (@TimKartawijaya), @ujjwal95, Zhida Zhang (@ZhangZhida) "],
["basics.html", "2 R Basics 2.1 Top 10 Essentials Checklist 2.2 Tips &amp; Tricks 2.3 Submitting Assignments 2.4 Getting help", " 2 R Basics So…there is soooo much to the world of R. Textbooks, cheatsheets, exercises, and other buzzwords full of resources you could go through. There are over 15000 packages on CRAN, the network through which R code and packages are distributed. It can be overwhelming. However, bear in mind that R is being used for a lot of different things, not all of which are relevant to EDAV. In an effort to get everyone on the same page, here is a checklist of essentials so you can get up and running with this course. The best resources are scattered in different places online, so bear with links to various sites depending on the topic. 2.1 Top 10 Essentials Checklist (r4ds = R for Data Science by Garrett Grolemund and Hadley Wickham, free online) Install R (r4ds) – You need to have this installed but you won’t open the application since you’ll be working in RStudio. If you already installed R, make sure you’re current! The latest version of R (as of 2019-10-07) is R 3.6.1 “Action of the Toes” released on 2019/07/05. Install RStudio (r4ds) – Download the free, Desktop version for your OS. Working in this IDE will make working in R much more enjoyable. As with R, stay current. RStudio is constantly adding new features. The latest version (as of 2019-10-07) is 1.2.5001. Get comfortable with RStudio – In this chapter of Bruno Rodriguez’s Modern R with the Tidyverse, you’ll learn about panes, options, getting help, keyboard shortcuts, projects, add-ins, and packages. Be sure to try out: Do some math in the console Create an R Markdown file (.Rmd) and render it to .html Install some packages like tidyverse or MASS Another great option for learning the IDE: Watch Writing Code in RStudio (RStudio webinar) Learn “R Nuts and Bolts” – Roger Peng’s chapter in R Programming will give you a solid foundation in the basic building blocks of R. It’s worth making the investing in understanding how R objects work now so they don’t cause you problems later. Focus on vectors and especially data frames; matrices and lists don’t come up often in data visualization. Get familiar with R classes: integer, numeric, character, and logical. Understand how factors work; they are very important for graphing. Learn some RMarkdown – For this class you will write assignments in R Markdown (stored as .Rmd files) and then render them into pdfs for submission. You can jump right in and open a new R Markdown file (File &gt; New File &gt; R Markdown…), and leave the Default Output Format as HTML. You will get a R Markdown template you can tinker with. Click the “knit” button and see what happens. For more detail, watch the RStudio webinar Getting Started with R Markdown Tidy up (r4ds) – Install the tidyverse, and get familiar with what it is. We will discuss differences between base R and the tidyverse in class. Learn ggplot2 basics (r4ds) – In class we will study the grammar of graphics on which ggplot2 is based, but it will help to familiarize yourself with the syntax in advance. Avail yourself of the “Data Visualization with ggplot2” cheatsheet by clicking “Help” “Cheatsheets…” within RStudio. Use RStudio projects (r4ds) – If you haven’t already, drink the Kool-Aid. Make each problem set a separate project. You will never have to worry about getwd() or setwd() again because everything will just be in the right places. Or watch the webinar: “Projects in RStudio” Learn the basic dplyr verbs for data manipulation (r4ds) – Concentrate on the main verbs: filter() (rows), select() (columns), mutate(), arrange() (rows), group_by(), and summarize(). Learn the pipe %&gt;% operator. Know how to tidy your data – The gather() function from the tidyr package will help you get your data in the right form for plotting. More on this in class. Check out these super cool animations, which follow a data frame as it is transformed by tidyr functions. General advice: don’t get caught up in the details. Keep a list of questions and move on. 2.2 Tips &amp; Tricks 2.2.1 Knitr Up your game with chunk options: check out the official list of options – and bookmark it! Some favorites are: warning=FALSE message=FALSE – especially useful when loading packages cache=TRUE – only changed chunks will be evaluated, be careful though since changes in dependencies will not be detected. fig.… options, see below 2.2.2 RStudio keyboard shortcuts option-command-i (“insert R chunk”) ```{r} ``` shift-command-M %&gt;% (“the pipe”) 2.2.3 Sizing figures (and more) Always use chunk options to size figures. You can set a default size in the YAML at the beginning of the .Rmd file as so: output: pdf_document: fig_height: 3 fig_width: 5 Another method is to click the gear ⚙️ next to the Knit button, then Output Options…, and finally the Figures tab. Then as needed override one or more defaults in particular chunks: {r, fig.width=4, fig.height=2} Figure related chunk options include fig.width, fig.height, fig.asp, and fig.align; there are many more. 2.2.4 Viewing plots in plot window Would you like your plots to appear in the plot window instead of below each chunk in the .Rmd file? Click ⚙️ and then Chunk Output in Console. 2.3 Submitting Assignments Here’s a quick run-down of how to submit your assignments using R Markdown and Knitr. Create R Markdown file with PDF output format: We will often provide you with a template, and feel free to add on to it directly, but make sure its output format is set to pdf_document. Write out your explanations and insert code chunks to answer the questions provided. If you want to make a new file, go to File &gt; New File &gt; R Markdown… and set the Default Output Format to PDF. Either way, the header of the .Rmd file should look something like this: Add PDF Dependencies: As stated when you create a new R Markdown file, the PDF output format requires TeX: Make sure you download TeX for your machine. Here are some Medium articles on the process of creating PDF reports (the articles cover starting from scratch with no installs at all, but you can skip over to installing TeX only): Mac OS Windows This can be a little complicated, but it will make that Knit button near the top of the IDE magically generate a PDF for you. If you are in a rush and want a shortcut, you can instead set the Default Output Format to HTML. When you open the file in your browser, you can save it as a PDF. It will not be as nicely formatted, but it will still work. 2.4 Getting help via https://dev.to/rly First off…breeeeeeathe. We can fix this. There are a bunch of resources out there that can help you. 2.4.1 Things to try Remember: Always try to help yourself! This article has a great list of tools to help you learn about anything you may be confused by. This includes learning about functions and packages as well as searching for info about a function/package/problem/etc. This is the perfect place to learn how to get the info you need. The RStudio Help menu (in the top toolbar) is a fantastic place to go for understanding/fixing any problems. There are links to documentation and manuals as well as cheatsheets and a lovely collection of keyboard shortcuts. Vignettes are a great way to learn about packages and how they work. Vignettes are like stylized manuals that can do a better job at explaining a package’s contents. For example, ggplot2 has a vignette on aesthetics called ggplot2-specs that talks about different ways you can map data to different formats. Typing browseVignettes() in the console will show you all the vignettes for all of the packages you have installed. You can also see vignettes by package by typing vignette(package = &quot;&lt;package_name&gt;&quot;) into the console. To run a specific vignette, use vignette(&quot;&lt;vignette_name&gt;&quot;). If the vignette can’t be resolved, include the package name as well: vignette(&quot;&lt;vignette_name&quot;, package = &quot;&lt;package_name&gt;&quot;) Don’t ignore errors. They are telling you so much! If you give up because red text showed up in your console, take the time to see what that red text is saying. Learn how to read errors and what they are telling you. They usually include where the problem happened and what R thinks the problem stems from. More Advanced: Learn to love debugger mode. Debugging can have a steep learning curve, but huge payoffs. Take a look at these videos about debugging with R. Topics include running the debugger, setting breakpoints, customizing preferences, and more. Note: R Markdown files have some limitations for debugging, as discussed in this article. You could also consider working out your code in a .R file before including it in your R Markdown homework submission. 2.4.2 Help me, R community! Relax. There are a bunch of people using the same tools you are. Your fellow classmates are a good place to start! Post questions to Piazza to see how they could help. There is a lot of great documentation on R and its functions/packages/etc. Get comfy with R Documentation and it will help you immensely. There is a vibrant RStudio Community page. Also, R likes twitter. Check out #rstats or maybe let Hadley Wickham know about a wonky error message. "],
["project.html", "3 Final Project Assignment 3.1 Overview 3.2 General info 3.3 Report format 3.4 Outline 3.5 Presentation style 3.6 Grading 3.7 Resources", " 3 Final Project Assignment 3.1 Overview This section goes over what’s expected for the final project, Spring 2019. General Note: Please note that this sheet cannot possibly cover all the “do’s and don’ts” of data analysis and visualization. You are expected to follow all of the best practices discussed in class throughout the semester. 3.2 General info 3.2.1 Goal The goal of this project is to perform an exploratory data analysis / create visualizations with data of your choosing in order to gain preliminary insights on questions of interest to you. 3.2.2 Teams For this assignment you will work in teams of 2 people. If you wish to select your own partner, please do so by Thursday, March 28, 2019. Piazza is a good place to post your interests and look for partners. After March 28th, partners will be randomly assigned. (This is actually a better option in terms of preparing yourself for a work environment with colleagues that you don’t know well.) If you wish to be assigned to a partner before that date so you can get started earlier (recommended) please email me. Once you know who you wish to work with, sign up in the People section of CourseWorks. Do not click the +Group button; rather, drag your names into one of the groups already created with the name “Final Project ”. (If you don’t follow these instructions and create your own group, it will not be linked to the Final Project assignment and therefore you won’t be able to submit your project properly as a team.) Once the groups are set up, we will ask for a short description of your project ideas, so start planning! 3.2.3 Topics The topic you choose is open-ended… choose something that you are intereted in and genuinely curious about! Think of some questions that you don’t know the answer to. Next look for data that might help you answer those questions. 3.2.4 Data The data can be pulled from multiple sources; it does not need to be a single dataset. Be sure to get data from the original source. For example, if you wish to work with data collected and distributed by the Centers for Disease Control, that is where you should go to access the data, not a third party that has posted the data. Avoid overused datasets (think Titanic) as well as those used in Kaggle (or similar) competitions. 3.2.5 Code All of your code should be stored on GitHub. We will spend some time in class learning how to set up a Git/GitHub workflow, but there is some leeway in terms of how you accomplish this. Keep your project organized. At a minimum, create a data/raw folder (maybe put in .gitignore), a data/tidy folder (or perhaps another name to indicate that you’ve worked on the data or subsetted it… it may not necessarily be tidy.), and analysis folder for your .Rmds and .html (output) files. Use may also have an R folder for preprocessing scripts or functions that don’t belong in the .Rmd file. In your report, include a link to the repo, as well as links to specific files as relevant. The static visualizations should be done in R, but other pieces, such as data importation and cleaning do not. 3.2.6 Analysis You have a lot of freedom to choose what to do, as long as you restrict yourselves to exploratory techniques (rather than modeling / prediction approaches). In addition, your analysis must be clearly documented and reproducible. 3.2.7 Feedback At any point, you may ask the TAs or the instructor (Joyce) for advice. Our primary role in this regard will be to provide general guidance on your choice of data / topic / direction. As always, you are encouraged to post specific questions to Piazza, particularly coding questions and issues. You may also volunteer to discuss your project with the class in order to get feedback–if you’d like to do this, email the instructor to schedule a date. 3.2.8 Peer review After final projects are turned in, you will be asked write peer reviews of other projects. Each individual will be assigned two project groups to review, and instructions will be provided. Note: part of the grade you receive for the class is based on the quality of review that you write, not on the feedback that your project receives. Your grade for the project (as for all other assignments for the class) will be determined solely by the instructor and TAs. 3.3 Report format The report should be about 15 pages with graphs, without code, approximately 5 pages for parts I.-IV. and 10 pages for parts V.-VII. You can check page length by looking at a browser print preview. All graphs should be accompanied by textual description / interpretation. You will likely produce many more graphs than you include in the report. You can keep them in separate files on GitHub, and link to them in the report as relevant. With the exception of the interactive part, your project should be submitted to CourseWorks as an .Rmd and .html, with graphs / output rendered. The output format in the YAML should be: output: html_document: code_folding: hide This format allows users to choose whether or not they wish to see the code. There will likely be pieces of your project that you can’t include in the .Rmd / .html file combo. In those cases, you should post those resources online and provide links to them in the report. This is particularly relevant to the interactive graph section. You will lose points if we have trouble reading your file, need to ask you to resubmit with graphs visible, if links are broken, or if we have other difficulties accessing your materials. It’s ok if code is in different files and different places, just make sure there are working links in your report to these locations. Note: Using Markdown + code chunks is supposed to make combining code, text and graphs easier. If it is making it more difficult, you are probably trying to do something that isn’t well suited to the tool set. Focus on the text and graphs, not the formatting. If you’re not sure if something is important to focus on or not, please ask. Advice: don’t wait to start writing. Your overall project will undoubtedly be better if you give up trying to get that last graph perfect or the last bit of analysis done and get to the writing! You are encouraged to be as intellectually honest as possible. That means pointing out flaws in your work, detailing obstacles, disagreements, decision points, etc. – the kinds of “behind-the-scene” things that are important but often left out of reports. You may use the first person (“I”/“We”) or specific team members’ names, as relevant. 3.4 Outline Your report should include the following sections, with headings (“I. Introduction”, etc.) as indicated: I. Introduction Explain why you chose this topic, and the questions you are interested in studying. Provide context for readers who are not familiar with the topic. II. Description of the data source Describe the data source: who is responsible for collecting the data? How is it collected? If there were a choice of options, explain how you chose. Provide some basic information about the dataset: types of variables, number of records, etc. Describe any known issues / problems about the data. (You should be able to write this section without actually working with the data.) III. Description of data import / cleaning / transformation Describe the process of getting the data into a form in which you could work with it in R. IV. Analysis of missing values Describe any patterns you discover in missing values. V. Results Provide a short nontechnical summary of the most revealing findings of your analysis written for a nontechnical audience. Take extra care to clean up your graphs, ensuring that best practices for presentation are followed, as described in the presentation style section below. Note: “Presentation” here refers to the style of graph, that is, graphs that are cleaned up for presentation, as opposed to the rough ones we often use for exploratory data analysis. You do not have to present your work to the class! VI. Interactive component Select one (or more) of your key findings to present in an interactive format. Be selective in the choices that you present to the user; the idea is that in 5-10 minutes, users should have a good sense of the question(s) that you are interested in and the trends you’ve identified in the data. In other words, they should understand the value of the analysis, be it business value, scientific value, general knowledge, etc. Interactive graphs must follow all of the best practices as with static graphs in terms of perception, labeling, accuracy, etc. You may choose the tool (D3, Shiny, or other) The complexity of your tool will be taken into account: we expect more complexity from a higher-level tool like Shiny than a lower-level tool like D3, which requires you to build a lot from scratch. Make sure that the user is clear on what the tool does and how to use it. Publish your graph somewhere on the web and provide a link in your report in the interactive section. The obvious choices are blockbuilder.org to create a block for D3, and shinyapps.io for Shiny apps but other options are fine. You are encouraged to share experiences on Piazza to help classmates with the publishing process. Note: the interactive component is worth approximately 25% of the final project grade. Therefore, do not spend 90% of your time on it… concentrate on the exploratory data analysis piece. VII. Conclusion Discuss limitations and future directions, lessons learned. 3.5 Presentation style As we’ve discussed throughout the semester, standards are higher for clarity in graphs designed to be shared with others. While “good enough” is our standard for EDA, we need to go the extra mile for presentation. The following is checklist of items to address to make your graphs presentation ready. (You do not have to worry about these items for the EDA section.) Title, axis labels, tick mark labels, and legends should be comprehensible (easy to understand) and legible (easy to read / decipher). Tick marks should not be labeled in scientific notation or with long strings of zeros, such as 3000000000. Instead, convert to smaller numbers and change the units: 3000000000 becomes “3” and the axis label “billions of views”. Units should be intuitive (An axis labeled in month/day/year format is intuitive; one labeled in seconds since January 1, 1970 is not.) The font size should be large enough to read clearly. The default in ggplot2 is generally too small. You can easily change it by passing the base font size to the theme, such as + theme_grey(16) (The default base font size is 11). The order of items on the axes and legends should be logical. (Alphabetical is usually not the best option.) Colors should be color-vision-deficiency-friendly. If categorical variable levels are long, set up the graph so the categorical variable is on the y-axis and the names are horizontal. A better option, if possible, is to shorten the names of the levels. Not all EDA graphs lend themselves to presentation, either because the graph form is hard to understand without practice or it’s not well labeled. The labeling problem can be solved by adding text in an image editor. The downside is that it is not reproducible. If you want to go this route, for the Mac, Keynote and Paintbrush are good, free options. Err on the side of simplicity. Don’t, for example, overuse color when it’s not necessary. Ask yourself: does color make this graph any clearer? If it doesn’t, leave it out. Test your graphs on nontechnical friends and family and ask for feedback. Above all, have fun with it 3.6 Grading We are more impressed by quality than quantity. In determining grades, we take the following into account: Originality Are your questions thought-provoking? Do they encourage the reader to think about the topic in a new way? Real world context Do your graphs and textual descriptions reflect a solid understanding of what your data mean? Is it clear why you are asking the questions that you are asking? Are your interpretations reasonable? Reproducibility Did you provide all of your code in a manner that will be easy for the reader to rerun your analysis, and include an explanation for any pieces that cannot be reproduced? Is your code clear? Multidimensionality Do you examine multidimensional relationships and present them clearly? Choice of graph forms Are your graph forms good choices for your data? Parameters / design decisions Have you made good choices in parameters, color, etc.? Standards Do your graphs meet presentation style standards? Interactive part How well does the interactive component connect with the goals of the project? Does it help the reader understand the main conclusions? 3.7 Resources “Tidy Tuesday Screencast: analyzing college major &amp; income data in R” David Robinson explores a dataset in R live, without looking at the data in advance. This may be helpful in figuring out how to get started. "],
["contribute.html", "4 Contribute to this Resource 4.1 Why contribute? 4.2 What to contribute 4.3 Ways to contribute 4.4 Submit an issue 4.5 GitHub only walkthrough 4.6 Resources", " 4 Contribute to this Resource 4.1 Why contribute? We don’t want edav.info/ to be just another resource. Rather, we want it to be your resource. If there are things that trip you up or cause you frustration, chances are you’re not alone. Everyone comes to this course with different backgrounds and expertise. Being able to collect all that knowledge in one place is this resource’s mission and you can help move that mission forward. 4.2 What to contribute The focus of edav.info is the coding aspect of data visualization using R. It is not meant to substitute for course lectures or provide much theory about exploratory data analysis and visualization. We want to make it easy for students to find what they need quickly to produce a particular kind of graph. We are open to a wide range of contributions but we do have some ground rules: We are happy to receive original work that will help someone else. For example, if you worked out how to use a function that is not documented well, or mastered a type of graph that was difficult to get right, please share! On the other hand, if good tutorials already exist, there’s no need to repeat the work. Just submit a link to the resource with a short description to the appropriate chapter. And of course don’t forget to cite your sources by providing links. 4.3 Ways to contribute There are many ways you can contribute: For large or small ideas, without providing the code submit an issue (very simple, much appreciated) For simple changes contribute on GitHub (this can all be done on GitHub.com… we’ve got a full walkthrough explaining how) For more complex changes install Git and and work locally. Detailed instructions can be found in our Git/GitHub Resources chapter (the next level, also much appreciated) – adventurous users may solve an open issue (more advanced/open-ended) 4.4 Submit an issue If your proposed change is more complex, consider letting us know by submitting an issue. Maybe you have a great idea for a brand new chapter, something we have not covered but would like to see here in this resource (a new chart page, say; or a walkthrough using a specific tool/package). It may be a little too complicated to contribute directly. What to do? Submit an issue, of course! Issues are tasks you can post to a GitHub repo that people can then take on and fix. They can be small (“this link is broken” / “add this resource”) or complex (“I would love to have a chapter on…” / “reformat this code chunk in this way”). Once posted, issues can be taken on by anyone. You do not have to know how to code up your issue; from fixing a bug to proposing a resource we should link to, we appreciate any feedback you have and will take it all into consideration. How to submit issues: Go to our GitHub repo and click on the Issues Tab Click on “New Issue” Propose your Issue and click “Submit new issue” That’s it! We appreciate your input and will take your issue into account in improving edav.info/ Notes about submitting issues: Make sure your changes are not already an open issue (so as not to have redundant issues) Please thoroughly explain your proposed change when posting a new issue Consider using labels to specify the kind of issue, such as “bug”, “enhancement”, “help wanted”, “question”, or create your own. For more info, please consider reading the Open Source Guide on how to contribute. 4.5 GitHub only walkthrough You will need to create a github.com account if you don’t have one, but you do not need to install git locally. Note: This is a full walkthough that follows a hypothetical student that spots a typo and uses a pull request to fix it. Although the instructions are written for proposing a change to edav.info, they apply to making changes to any repo. Navigate to the file you wish to edit, and click the pencil button: Do not click the fork button on the home page of the repo. Then jump ahead to Step 4 below. One way to contribute to edav.info is to contribute directly by editing a chapter. At the top of every page of this resource, you will see an icon that looks like this: . Clicking it will open a new tab where you can edit the markdown for that page on our GitHub repo and submit your change as a pull request. Essentially, you will create a copy of our repo, make your desired changes, and suggest to us that we include them. If we approve of your changes, they will be rendered and published to the site. Contributing directly in this manner works best if the change you are proposing is something relatively small, such as a typo/grammatical error or an unclear phrasing/explanation. In general, it doesn’t work well to propose changes to code directly on GitHub. Ok, we’re good to go. Get ready to hit lots of big green buttons! One last thing: remember there’s no way to make changes to someone else’s repo unless you submit a pull request and the owner merges it. So don’t be afraid to act like the repo owner and click to edit files. You will be editing a copy of the files; it’s ok! Let’s find something to change. I’m pretty sure they meant to write “repository” here. Oops. Let’s fix it for them! That’s not how you spell “repository”! Let’s fix it. To make the fix, we click on the edit icon, , at the top of the page. This will take us to their GitHub repo, where all the code for this resource is stored. Note: You need to have a valid GitHub account to contribute. In this example, we are using a dummy account called excited-student so if you see it in a screenshot, know that it would be replaced by your own username. Hit this icon to go to GitHub. This is our first edit to the repo, so GitHub shows us a page like the one below. No worries! We just hit the big green button labeled Fork this repository and propose changes and we’ll be good to go (as you will see, big green buttons are our friends). Note: you will not have to fork the repo every time. If you propose another change in the future, the edit icon, , will jump you directly to this point of the walkthrough. Haven’t forked the repo before? No worries; the big green button will solve everything. Now that we have successfully forked the repo, we can see the code for the page we want to edit. Note: That little blue blurb at the top is spelling out what is happening/going to happen: we have made a copy of a repo because we don’t have write access to it . So, after we make our change on this page, we will inform the owners of the repo about our edits by using a pull request. GitHub can be super overwhelming, but it will try its darndest to inform you what will happen along the way. Ready to edit the code. The blue blurb is worth reading. Let’s fix that embarrassing typo! We update the code right in this editor, include an explanation for what we changed/why we made the change, and then hit the big green button labeled Propose file change. Gotta love those big green buttons! Make your edits, include a quick explanation, and hit the big green button. Now GitHub is once again helping out by letting us review the changes we made. On this page we can review our proposed changes by scrolling down and looking at the diffs. Our fix is very simple so there isn’t much to see. Once again, we are going to push the big green button, this time labeled Create pull request. This will start the process of letting the edav.info/ people know that we would like them to include our changes (in git-speak, we are requesting that the edav.info/ people do a git merge to update their files with our proposed changes.) Note that the proposed changes are in a branch called patch-1 on our repo; we are asking to merge them into the master branch on their repo. Chance to review your changes. Once satisfied, hit the big green button to start a pull request. Here we are at the pull request page. Notice the green checkmark that says “Able to merge” (a good sign that everything is going smoothly). Now we explain our pull request with some comments and, once again, hit the big green button labeled Create pull request. Note: You may be asking, “Why do I have to type this explanation in again?”. This is because the explanation we wrote in Step 5 (where we edited the file) is a commit. We could have had multiple commits at once that we wanted to bundle into one pull request. This step is a way to explain the pull request as a whole. It is redundant for us because our change is so small and only has one commit. Still totally lost? This GitHub Guide on Understanding the GitHub Flow is an incredibly helpful read and our GitHub Resources page also has a lot of helpful links. Explain your pull request and hit the big green button. Congratulations are in order! We have successfully opened a pull request on a GitHub repo! Now one of the repo owners (like the guy writing this tutorial, for example ) has to decide if they want to include your pull request or not. In this case they’ll certainly approve it, but know that they may decide against adding your changes. For more info, read the section of the Open Source Guides on what happens after you submit a contribution. Note: Be aware that the icon shown below may initially be yellow to signal that some tests are being performed to check the conflicts of your proposal with the original repo. It should turn green if everything passes. We did it! Now the maintainers will review our changes and get back to us… And now we wait… via GIPHY What’s this!? We have received an email from one of the repo owners, Zach Bogart. And it says that they merged the change! Huzzah! We click on the number to take us back to the pull request we opened. We got an email! And it says they merged! Click that number to see the updated pull request. Here we are at the updated pull request page. Notice that everything has turned purple. Purple is the best color to see on GitHub; it’s the color of victory. It signals that our pull request was merged with the repo, meaning our change has become part of the repo! Also, notice the button that says Delete branch. Since all the work on our branch was merged with the repo, it has served its purpose and can be deleted safely. Everything is purple! Woot! Can safely delete our branch Now if we go back to the main page of the repo, we can see our merge was the most recent addition. And, if we scroll down, we will see that github_resources.Rmd, the file we edited, has been updated recently and it shows our commit message “fix typo”. We did it! Let’s check out the site to see our change published for the whole internet to see! Look! There’s our merged pull request added to the repo! And the edits we made to github_resources.Rmd! There it is! We go back to the page we edited and now our typo fix has been included!Note: The changes will take several minutes to appear on the site after notification of a successful merge. This is because we use Travis CI on the backend of our repo and it takes a little time for it to re-render the site pages. If you want to learn more about how you can use Travis CI to auto-magically generate your work, checkout our section on Hooking Up Travis to a GitHub bookdown book in the Publishing Resources page. Look at that! It’s published! So many exclamation points!!! We contributed to a GitHub repo! Hooray! Time to celebrate! via GIPHY Looking ahead: the next edit If you have a second edit to propose, simply follow the instructions again. As noted above, the second time through you will not be asked to fork the repo again. If you look closely at the pull request for the second edit, the branch to be merged will be named patch-2 instead of patch-1. Although GitHub keeps mentioning “your fork” as you proceed through the process, this is not really something that you have to concern yourself with. In fact, you’re better off not! In fact, you should stay away from your fork – that is, your copy of the EDAV repo in your GitHub account, because it will inevitably get behind the main one and cause you trouble if you work on the old version and then try to create a pull request. So, the bottom line is, each time you have an edit to propose, go directly to either edav.info or https://github.com/jtr13/EDAV and start the editing process there, not on your fork!. 4.6 Resources Our GitHub repo: Link to the GitHub repository for edav.info/ Open Source Guide: Fantastic guide on how to contribute to projects like this one "],
["histo.html", "5 Chart: Histogram 5.1 Overview 5.2 tl;dr 5.3 Simple examples 5.4 Theory 5.5 Types of histograms 5.6 Parameters 5.7 Interactive histograms with ggvis 5.8 External resources", " 5 Chart: Histogram 5.1 Overview This section covers how to make histograms. 5.2 tl;dr Gimme a full-fledged example! Here’s an application of histograms that looks at how the beaks of Galapagos finches changed due to external factors: And here’s the code: library(Sleuth3) # data library(ggplot2) # plotting # load data finches &lt;- Sleuth3::case0201 # finch histograms by year with overlayed density curves ggplot(finches, aes(x = Depth, y = ..density..)) + # plotting geom_histogram(bins = 20, colour = &quot;#80593D&quot;, fill = &quot;#9FC29F&quot;, boundary = 0) + geom_density(color = &quot;#3D6480&quot;) + facet_wrap(~Year) + # formatting ggtitle(&quot;Severe Drought Led to Finches with Bigger Chompers&quot;, subtitle = &quot;Beak Depth Density of Galapagos Finches by Year&quot;) + labs(x = &quot;Beak Depth (mm)&quot;, caption = &quot;Source: Sleuth3::case0201&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?Sleuth3::case0201 into the console. 5.3 Simple examples Whoa whoa whoa! Much simpler please! Let’s use a very simple dataset: # store data x &lt;- c(50, 51, 53, 55, 56, 60, 65, 65, 68) 5.3.1 Histogram using base R # plot data hist(x, col = &quot;lightblue&quot;, main = &quot;Base R Histogram of x&quot;) For the Base R histogram, it’s advantages are in it’s ease to setup. In truth, all you need to plot the data x in question is hist(x), but we included a little color and a title to make it more presentable. Full documentation on hist() can be found here 5.3.2 Histogram using ggplot2 # import ggplot library(ggplot2) # must store data as dataframe df &lt;- data.frame(x) # plot data ggplot(df, aes(x)) + geom_histogram(color = &quot;grey&quot;, fill = &quot;lightBlue&quot;, binwidth = 5, center = 52.5) + ggtitle(&quot;ggplot2 histogram of x&quot;) The ggplot version is a little more complicated on the surface, but you get more power and control as a result. Note: as shown above, ggplot expects a dataframe, so if you are getting an error where “R doesn’t know what to do” like this: ggplot dataframe error make sure you are using a dataframe. 5.4 Theory Generally speaking, the histogram is one of many options for displaying continuous data. The histogram is clear and quick to make. Histograms are relatively self-explanatory: they show your data’s empirical distribution within a set of intervals. Histograms can be employed on raw data to quickly show the distribution without much manipulation. Use a histogram to get a basic sense of the distribution with minimal processing necessary. For more info about histograms and continuous variables, check out Chapter 3 of the textbook. 5.5 Types of histograms Use a histogram to show the distribution of one continuous variable. The y-scale can be represented in a variety of ways to express different results: 5.5.1 Frequency or count y = number of values that fall in each bin 5.5.2 Relative frequency historgram y = number of values that fall in each bin / total number of values 5.5.3 Cumulative frequency histogram y = total number of values &lt;= (or &lt;) right boundary of bin 5.5.4 Density y = relative frequency / binwidth 5.6 Parameters 5.6.1 Bin boundaries Be mindful of the boundaries of the bins and whether a point will fall into the left or right bin if it is on a boundary. # format layout op &lt;- par(mfrow = c(1, 2), las = 1) # right closed hist(x, col = &quot;lightblue&quot;, ylim = c(0, 4), xlab = &quot;right closed ex. (55, 60]&quot;, font.lab = 2) # right open hist(x, col = &quot;lightblue&quot;, right = FALSE, ylim = c(0, 4), xlab = &quot;right open ex. [55, 60)&quot;, font.lab = 2) 5.6.2 Bin number The default bin number of 30 in ggplot2 is not always ideal, so consider altering it if things are looking strange. You can specify the width explicitly with binwidth or provide the desired number of bins with bins. # default...note the pop-up about default bin number ggplot(finches, aes(x = Depth)) + geom_histogram() + ggtitle(&quot;Default with pop-up about bin number&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Here are examples of changing the bins using the two ways described above: # using binwidth p1 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(binwidth = 0.5, boundary = 6) + ggtitle(&quot;Changed binwidth value&quot;) # using bins p2 &lt;- ggplot(finches, aes(x = Depth)) + geom_histogram(bins = 48, boundary = 6) + ggtitle(&quot;Changed bins value&quot;) # format plot layout library(gridExtra) grid.arrange(p1, p2, ncol = 2) 5.6.3 Bin alignment Make sure the axes reflect the true boundaries of the histogram. You can use boundary to specify the endpoint of any bin or center to specify the center of any bin. ggplot2 will be able to calculate where to place the rest of the bins (Also, notice that when the boundary was changed, the number of bins got smaller by one. This is because by default the bins are centered and go over/under the range of the data.) df &lt;- data.frame(x) # default alignment ggplot(df, aes(x)) + geom_histogram(binwidth = 5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Default Bin Alignment&quot;) # specify alignment with boundary p3 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, boundary = 60, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Bin Alignment Using boundary&quot;) # specify alignment with center p4 &lt;- ggplot(df, aes(x)) + geom_histogram(binwidth = 5, center = 67.5, fill = &quot;lightBlue&quot;, col = &quot;black&quot;) + ggtitle(&quot;Bin Alignment Using center&quot;) # format layout library(gridExtra) grid.arrange(p3, p4, ncol = 2) Note: Don’t use both boundary and center for bin alignment. Just pick one. 5.7 Interactive histograms with ggvis The ggvis package is not currently in development, but does certain things very well, such as adjusting parameters of a histogram interactively while coding. Since images cannot be shared by knitting (as with other packages, such as plotly), we present the code here, but not the output. To try them out, copy and paste into an R session. 5.7.1 Change binwidth interactively library(tidyverse) library(ggvis) faithful %&gt;% ggvis(~eruptions) %&gt;% layer_histograms(fill := &quot;lightblue&quot;, width = input_slider(0.1, 2, value = .1, step = .1, label = &quot;width&quot;)) 5.7.2 GDP example df &lt;-read.csv(&quot;countries2012.csv&quot;) df %&gt;% ggvis(~GDP) %&gt;% layer_histograms(fill := &quot;green&quot;, width = input_slider(500, 10000, value = 5000, step = 500, label = &quot;width&quot;)) 5.7.3 Change center interactively df &lt;- data.frame(x = c(50, 51, 53, 55, 56, 60, 65, 65, 68)) df %&gt;% ggvis(~x) %&gt;% layer_histograms(fill := &quot;red&quot;, width = input_slider(1, 10, value = 5, step = 1, label = &quot;width&quot;), center = input_slider(50, 55, value = 52.5, step = .5, label = &quot;center&quot;)) 5.7.4 Change center (with data values shown) df &lt;- data.frame(x = c(50, 51, 53, 55, 56, 60, 65, 65, 68), y = c(.5, .5, .5, .5, .5, .5, .5, 1.5, .5)) df %&gt;% ggvis(~x, ~y) %&gt;% layer_histograms(fill := &quot;lightcyan&quot;, width = 5, center = input_slider(45, 55, value = 45, step = 1, label = &quot;center&quot;)) %&gt;% layer_points(fill := &quot;blue&quot;, size := 200) %&gt;% add_axis(&quot;x&quot;, properties = axis_props(labels = list(fontSize = 20))) %&gt;% scale_numeric(&quot;x&quot;, domain = c(46, 72)) %&gt;% add_axis(&quot;y&quot;, values = 0:3, properties = axis_props(labels = list(fontSize = 20))) 5.7.5 Change boundary interactively df %&gt;% ggvis(~x) %&gt;% layer_histograms(fill := &quot;red&quot;, width = input_slider(1, 10, value = 5, step = 1, label = &quot;width&quot;), boundary = input_slider(47.5, 50, value = 50, step = .5, label = &quot;boundary&quot;)) 5.8 External resources hist documentation: base R histogram documentation page. ggplot2 cheatsheet: Always good to have close by. "],
["box.html", "6 Chart: Boxplot 6.1 tl;dr 6.2 Simple examples 6.3 Theory 6.4 When to use 6.5 Considerations 6.6 External resources", " 6 Chart: Boxplot 6.1 tl;dr I want a nice example and I want it NOW! Here’s a look at the weights of newborn chicks split by the feed supplement they received: And here’s the code: library(ggplot2) # boxplot by feed supplement ggplot(chickwts, aes(x = reorder(feed, -weight, median), y = weight)) + # plotting geom_boxplot(fill = &quot;#cc9a38&quot;, color = &quot;#473e2c&quot;) + # formatting ggtitle(&quot;Casein Makes You Fat?!&quot;, subtitle = &quot;Boxplots of Chick Weights by Feed Supplement&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;, caption = &quot;Source: datasets::chickwts&quot;) + theme_grey(16) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?datasets::chickwts into the console. 6.2 Simple examples Okay…much simpler please. 6.2.1 Single boxplots Base R will give you a quick boxplot of a vector or a single column of a data frame with very little typing: # vector boxplot(rivers) Or, the horizontal version: # single column of a data frame boxplot(chickwts$weight, horizontal = TRUE) Creating a single boxplot in ggplot2 is somewhat problematic. (The joke is that it’s the package author’s way of saying that if you only have one group, make a histogram instead!) If you only include one aesthetic mapping, it will be assumed to the x (group) variable and you will get an error: ggplot(chickwts, aes(weight)) + geom_boxplot() Error: stat_boxplot requires the following missing aesthetics: y This can be remedied by adding y = to indicate that weight is the numeric variable, but you’ll still get a meaningless x-axis: ggplot(chickwts, aes(y = weight)) + geom_boxplot() + theme_grey(16) # make all font sizes larger (default is 11) Another, cleaner approach is to create a name for the single group as the x aesthetic and remove the x-axis label: ggplot(chickwts, aes(x = &quot;all 71 chickens&quot;, y = weight)) + geom_boxplot() + xlab(&quot;&quot;) + theme_grey(16) 6.2.2 Multiple boxplots using ggplot2 To create multiple boxplots with ggplot2, your data frame needs to be tidy, that is you need to have a column with levels of the grouping variable. It can be be factor, character, or integer class. str(chickwts) ## &#39;data.frame&#39;: 71 obs. of 2 variables: ## $ weight: num 179 160 136 227 217 168 108 124 143 140 ... ## $ feed : Factor w/ 6 levels &quot;casein&quot;,&quot;horsebean&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... We see that chickwts is in the right form: we have a feed column with six factor levels, so we can set the the x aesthetic to feed. We also order the boxplots by decreasing median weight: ggplot(chickwts, aes(x = reorder(feed, -weight, median), y = weight)) + geom_boxplot() + xlab(&quot;feed type&quot;) + theme_grey(16) Data frames that contain a separate column of values for each desired boxplot must be tidied first. (For more detail on using tidy::gather(), see this tutorial.) library(tidyverse) head(attitude) ## rating complaints privileges learning raises critical advance ## 1 43 51 30 39 61 92 45 ## 2 63 64 51 54 63 73 47 ## 3 71 70 68 69 76 86 48 ## 4 61 63 45 47 54 84 35 ## 5 81 78 56 66 71 83 47 ## 6 43 55 49 44 54 49 34 tidyattitude &lt;- attitude %&gt;% gather(key = &quot;question&quot;, value = &quot;rating&quot;) head(tidyattitude) ## question rating ## 1 complaints 51 ## 2 complaints 64 ## 3 complaints 70 ## 4 complaints 63 ## 5 complaints 78 ## 6 complaints 55 Now we’re ready to plot: ggplot(tidyattitude, aes(reorder(question, -rating, median), rating)) + geom_boxplot() + xlab(&quot;question short name&quot;) + theme_grey(16) 6.3 Theory Here’s a quote by Hadley Wickham that sums up boxplots nicely: The boxplot is a compact distributional summary, displaying less detail than a histogram or kernel density, but also taking up less space. Boxplots use robust summary statistics that are always located at actual data points, are quickly computable (originally by hand), and have no tuning parameters. They are particularly useful for comparing distributions across groups. - Hadley Wickham Another important use of the boxplot is in showing outliers. A boxplot shows how much of an outlier a data point is with quartiles and fences. Use the boxplot when you have data with outliers so that they can be exposed. What it lacks in specificity it makes up with its ability to clearly summarize large data sets. For more info about boxplots and continuous variables, check out Chapter 3 of the textbook. 6.4 When to use Boxplots should be used to display continuous variables. They are particularly useful for identifying outliers and comparing different groups. Aside: Boxplots may even help you convince someone you are their outlier (If you like it when people over-explain jokes, here is why that comic is funny.). 6.5 Considerations 6.5.1 Flipping orientation Often you want boxplots to be horizontal. Super easy to do in ggplot2: just tack on + coord_flip() and remove the - from the reordering so that the factor level with the highest median will be on top: ggplot(tidyattitude, aes(reorder(question, rating, median), rating)) + geom_boxplot() + coord_flip() + xlab(&quot;question short name&quot;) + theme_grey(16) Note that switching x and y insteading of using coord_flip() doesn’t work! ggplot(tidyattitude, aes(rating, reorder(question, rating, median))) + geom_boxplot() + ggtitle(&quot;This is not what we wanted!&quot;) + ylab(&quot;question short name&quot;) + theme_grey(16) 6.5.2 NOT for categorical data Boxplots are great, but they do NOT work with categorical data. Make sure your variable is continuous before using boxplots. The data in this example are variables from the pisaitems dataset in the likert package with ratings of 1, 2, 3 or 4: head(pisa, 4) ## ST24Q01 ST24Q02 ST24Q03 ST24Q04 ST24Q05 ST24Q06 ## 1 2 4 4 1 4 1 ## 2 3 1 1 4 1 3 ## 3 4 1 1 3 1 4 ## 4 2 2 3 1 2 2 Creating a boxplot from this data is a good example of what not to do: 6.6 External resources Tukey, John W. 1977. Exploratory Data Analysis. Addison-Wesley. (Chapter 2): the primary source in which boxplots are first presented. Article on boxplots with ggplot2: An excellent collection of code examples on how to make boxplots with ggplot2. Covers layering, working with legends, faceting, formatting, and more. If you want a boxplot to look a certain way, this article will help. Boxplots with plotly package: boxplot examples using the plotly package. These allow for a little interactivity on hover, which might better explain the underlying statistics of your plot. ggplot2 Boxplot: Quick Start Guide: Article from STHDA on making boxplots using ggplot2. Excellent starting point for getting immediate results and custom formatting. ggplot2 cheatsheet: Always good to have close by. Hadley Wickhan and Lisa Stryjewski on boxplots: good for understanding basics of more complex boxplots and some of the history behind them. "],
["violin.html", "7 Chart: Violin Plot 7.1 Overview 7.2 Some Examples in R 7.3 Adding Statistics to the Violin Plot 7.4 Description 7.5 When to use 7.6 External Resources", " 7 Chart: Violin Plot This chapter originated as a community contribution created by AshwinJay101 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 7.1 Overview This section covers how to make violin plots. 7.2 Some Examples in R Let’s use the chickwts dataset from the datasets package to plot a violin plot using ggplot2. Here’s the code for that: # import ggplot and the Datasets Package library(datasets) library(ggplot2) supps &lt;- c(&quot;horsebean&quot;, &quot;linseed&quot;, &quot;soybean&quot;, &quot;meatmeal&quot;, &quot;sunflower&quot;, &quot;casein&quot;) # plot data ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) 7.3 Adding Statistics to the Violin Plot 7.3.1 Adding the median and the interquartile range We can add the median and the interquartile range to the violin plot ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) + geom_boxplot(width=0.1) To get the result, we just add a boxplot geom. 7.3.2 Displaying data as dots ggplot(chickwts, aes(x = factor(feed, levels = supps), y = weight)) + # plotting geom_violin(fill = &quot;lightBlue&quot;, color = &quot;#473e2c&quot;) + labs(x = &quot;Feed Supplement&quot;, y = &quot;Chick Weight (g)&quot;) + geom_dotplot(binaxis=&#39;y&#39;, dotsize=0.5, stackdir=&#39;center&#39;) 7.4 Description Violin plots are similar to box plots. The advantage they have over box plots is that they allow us to visualize the distribution of the data and the probability density. We can think of violin plots as a combination of boxplots and density plots. This plot type allows us to see whether the data is unimodal, bimodal or multimodal. These simple details will be hidden in the boxplot. The distribution can be seen through the width of the violin plot. 7.5 When to use Violin plots should be used to display continuous variables only. 7.6 External Resources ggplot2 Violin Plot: Excellent resource for showing the various customizations that can be added to the violin plot. "],
["ridgeline.html", "8 Chart: Ridgeline Plots 8.1 Overview 8.2 tl;dr 8.3 Simple examples 8.4 Ridgeline Plots using ggridge 8.5 When to Use 8.6 Considerations 8.7 External Resources", " 8 Chart: Ridgeline Plots This chapter originated as a community contribution created by nehasaraf1994 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 8.1 Overview This section covers how to make ridgeline plots. 8.2 tl;dr I want a nice example and I want it NOW! Here’s a look at the dose of theophylline administered orally to the subject on which the concentration of theophylline is observed: Here is the code: library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) Theoph_data &lt;- Theoph ggplot(Theoph_data, aes(x=Dose,y=Subject,fill=Subject))+ geom_density_ridges_gradient(scale = 4, show.legend = FALSE) + theme_ridges() + scale_y_discrete(expand = c(0.01, 0)) + scale_x_continuous(expand = c(0.01, 0)) + labs(x = &quot;Dose of theophylline(mg/kg)&quot;,y = &quot;Subject #&quot;) + ggtitle(&quot;Density estimation of dosage given to various subjects&quot;) + theme(plot.title = element_text(hjust = 0.5)) For more info on this dataset, type ?datasets::Theoph into the console. 8.3 Simple examples Okay…much simpler please. Let’s use the Orange dataset from the datasets package: library(&quot;datasets&quot;) head(Orange, n=5) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 8.4 Ridgeline Plots using ggridge library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(Orange, aes(x=circumference,y=Tree,fill = Tree))+ geom_density_ridges(scale = 2, alpha=0.5) + theme_ridges()+ scale_fill_brewer(palette = 4)+ scale_y_discrete(expand = c(0.8, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Circumference at Breast Height&quot;, y=&quot;Tree with ordering of max diameter&quot;)+ ggtitle(&quot;Density estimation of circumference of different types of Trees&quot;)+ theme(plot.title = element_text(hjust = 0.5)) ggridge uses two main geoms to plot the ridgeline density plots: “geom_density_ridges” and “geom_ridgeline”. They are used to plot the densities of categorical variable factors and see their distribution over a continuous scale. 8.5 When to Use Ridgeline plots can be used when a number of data segments have to be plotted on the same horizontal scale. It is presented with slight overlap. Ridgeline plots are very useful to visualize the distribution of a categorical variable over time or space. A good example using ridgeline plots will be a great example is visualizing the distribution of salary over different departments in a company. 8.6 Considerations The overlapping of the density plot can be controlled by adjusting the value of scale. Scale defines how much the peak of the lower curve touches the curve above. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) OrchardSprays_data &lt;- OrchardSprays ggplot(OrchardSprays_data, aes(x=decrease,y=treatment,fill=treatment))+ geom_density_ridges_gradient(scale=3) + theme_ridges()+ scale_y_discrete(expand = c(0.3, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Response in repelling honeybees&quot;,y=&quot;Treatment&quot;)+ ggtitle(&quot;Density estimation of response by honeybees to a treatment for scale=3&quot;)+ theme(plot.title = element_text(hjust = 0.5)) ggplot(OrchardSprays_data, aes(x=decrease,y=treatment,fill=treatment))+ geom_density_ridges_gradient(scale=5) + theme_ridges()+ scale_y_discrete(expand = c(0.3, 0)) + scale_x_continuous(expand = c(0.01, 0))+ labs(x=&quot;Response in repelling honeybees&quot;,y=&quot;Treatment&quot;)+ ggtitle(&quot;Density estimation of response by honeybees to a treatment for scale=5&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Ridgeline plots can also be used to plot histograms on the common horizontal axis rather than density plots. But doing that may not give us any valuable results. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(InsectSprays, aes(x = count, y = spray, height = ..density.., fill = spray)) + geom_density_ridges(stat = &quot;binline&quot;, bins = 20, scale = 0.7, draw_baseline = FALSE) If the same thing is done in ridgeline plots, it gives better results. library(&quot;ggridges&quot;) library(&quot;tidyverse&quot;) ggplot(InsectSprays, aes(x=count,y=spray,fill=spray))+ geom_density_ridges_gradient() + theme_ridges()+ labs(x=&quot;Count of Insects&quot;,y=&quot;Types of Spray&quot;)+ ggtitle(&quot;The counts of insects treated with different insecticides.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 8.7 External Resources Introduction to ggridges: An excellent collection of code examples on how to make ridgeline plots with ggplot2. Covers every parameter of ggridges and how to modify them for better visualization. If you want a ridgeline plot to look a certain way, this article will help. Article on ridgeline plots with ggplot2: Few examples using different examples. Great for starting with ridgeline plots. History of Ridgeline plots: To refer to the theory of ridgeline plots. "],
["qqplot.html", "9 Chart: QQ-Plot 9.1 Introduction 9.2 Interpreting qqplots 9.3 Normal or not (examples using qqnorm) 9.4 Different kinds of qqplots 9.5 qqplot using ggplot 9.6 References", " 9 Chart: QQ-Plot This chapter originated as a community contribution created by hao871563506 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 9.1 Introduction In statistics, a Q-Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile. 9.2 Interpreting qqplots 9.3 Normal or not (examples using qqnorm) 9.3.1 Normal qqplot x &lt;- rnorm(1000, 50, 10) qqnorm(x) qqline(x, col = &quot;red&quot;) The points seem to fall along a straight line. Notice the x-axis plots the theoretical quantiles. Those are the quantiles from the standard Normal distribution with mean 0 and standard deviation 1. 9.3.2 Non-normal qqplot x &lt;- rexp(1000, 5) qqnorm(x) qqline(x, col = &quot;red&quot;) Notice the points form a curve instead of a straight line. Normal Q-Q plots that look like this usually mean your sample data are skewed. 9.4 Different kinds of qqplots The following graph is a conclusion of all the kinds of qqplot: via Stack Exchange Normal qqplot: The normal distribution is symmetric, so it has no skew (the mean is equal to the median). Right skewed qqplot: Right-skew is also known as positive skew. Left skewed qqplot: Left-skew is also known as negative skew. Light tailed qqplot: meaning that compared to the normal distribution there is little more data located at the extremes of the distribution and less data in the center of the distribution. Heavy tailed qqplot: meaning that compared to the normal distribution there is much more data located at the extremes of the distribution and less data in the center of the distribution. Biomodel qqplot: illustrate a bimodal distribution. 9.5 qqplot using ggplot In order to use ggplot2 to plot a qqplot, we must use a dataframe, so here we convert it to one. We can see that using ggplot to plot a qqplot has a similar outcome as using qqnorm library(ggplot2) x &lt;- rnorm(1000, 50, 10) x &lt;- data.frame(x) ggplot(x, aes(sample = x)) + stat_qq() + stat_qq_line() However, when we need to plot different groups, ggplot will be very helpful with its coloring by factor. library(ggplot2) ggplot(mtcars, aes(sample = mpg, colour = factor(cyl))) + stat_qq() + stat_qq_line() 9.6 References Understanding Q-Q Plots: A discussion from the University of Virginia Library on qqplots. How to interpret a QQ plot: Another resource for interpreting qqplots. A QQ Plot Dissection Kit: An excellent walkthrough on qqplots by Sean Kross. Probability plotting methods for the analysis of data: Paper on plotting techniques, which discusses qqplots. (Wilk, M.B.; Gnanadesikan, R. (1968)) QQ-Plot Wiki: Wikipedia entry on qqplots "],
["bar.html", "10 Chart: Bar Chart 10.1 Overview 10.2 tl;dr 10.3 Simple examples 10.4 Theory 10.5 When to use 10.6 Considerations 10.7 Modifications 10.8 External resources", " 10 Chart: Bar Chart 10.1 Overview This section covers how to make bar charts 10.2 tl;dr I want a nice example. Not tomorrow, not after breakfast. NOW! Here’s a bar chart showing the survival rates of passengers aboard the RMS Titanic: And here’s the code: library(datasets) # data library(ggplot2) # plotting library(dplyr) # manipulation # Combine Children and Adult stats together ship_grouped &lt;- as.data.frame(Titanic) %&gt;% group_by(Class, Sex, Survived) %&gt;% summarise(Total = sum(Freq)) ggplot(ship_grouped, aes(x = Survived, y = Total, fill = Sex)) + geom_bar(position = &quot;dodge&quot;, stat = &quot;identity&quot;) + geom_text(aes(label = Total), position = position_dodge(width = 0.9), vjust = -0.4, color = &quot;grey68&quot;) + facet_wrap(~Class) + # formatting ylim(0, 750) + ggtitle(&quot;Don&#39;t Be A Crew Member On The Titanic&quot;, subtitle = &quot;Survival Rates of Titanic Passengers by Class and Gender&quot;) + scale_fill_manual(values = c(&quot;#b2df8a&quot;, &quot;#a6cee3&quot;)) + labs(y = &quot;Passenger Count&quot;, caption = &quot;Source: titanic::titanic_train&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?datasets::Titanic into the console. 10.3 Simple examples My eyes were bigger than my stomach. Much simpler please! Let’s use the HairEyeColor dataset. To start, we will just look at the different categories of hair color among females: colors &lt;- as.data.frame(HairEyeColor) # just female hair color, using dplyr colors_female_hair &lt;- colors %&gt;% filter(Sex == &quot;Female&quot;) %&gt;% group_by(Hair) %&gt;% summarise(Total = sum(Freq)) # take a look at data head(colors_female_hair) ## # A tibble: 4 x 2 ## Hair Total ## &lt;fct&gt; &lt;dbl&gt; ## 1 Black 52 ## 2 Brown 143 ## 3 Red 37 ## 4 Blond 81 Now let’s make some graphs with this data. 10.3.1 Bar graph using base R barplot(colors_female_hair[[&quot;Total&quot;]], names.arg = colors_female_hair[[&quot;Hair&quot;]], main = &quot;Bar Graph Using Base R&quot;) We recommend using Base R only for simple bar graphs for yourself. Like all of Base R, it is simple to setup. Note: Base R expects a vector or matrix, hence the double brackets in the barplot call (gets columns as lists). 10.3.2 Bar graph using ggplot2 library(ggplot2) # plotting ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) Bar plots are very easy in ggplot2. You pass in a dataframe and let it know which parts you want to map to different aesthetics. Note: In this case, we have a table of values and want to plot them as explicit bar heights. Because of this, we specify the y aesthetic as the Total column, but we also have to specify stat = &quot;identity&quot; in geom_bar() so it knows to plot them correctly. Often you will have datasets where each row is one observation and you want to group them into bars. In that case, the y aesthetic and stat = &quot;identity&quot; do not have to be specified. 10.4 Theory For more info about plotting categorical data, check out Chapter 4 of the textbook. 10.5 When to use Bar Charts are best for categorical data. Often you will have a collection of factors that you want to split into different groups. 10.6 Considerations 10.6.1 Not for continuous data If you are finding that your bar graphs aren’t looking right, make sure your data is categorical and not continuous. If you want to plot continuous data using bars, that is what histograms are for! 10.7 Modifications These modifications assume you are using ggplot2. 10.7.1 Flip Bars To flip the orientation, just tack on coord_flip(): ggplot(colors_female_hair, aes(x = Hair, y = Total)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Bar Graph Using ggplot2&quot;) + coord_flip() 10.7.2 Reorder the bars With both base R and ggplot2 bars are drawn in alphabetical order for character data and in the order of factor levels for factor data. However, since the default order of levels for factor data is alphabetical, the bars will be alphabetical in both cases. Please see this tutorial for a detailed explanation on how bars should be ordered in a bar chart, and how the forcats package can help you accomplish the reordering. 10.7.3 Facet Wrap You can split the graph into small multiples using facet_wrap() (don’t forget the tilde, ~): ggplot(colors, aes(x = Sex, y = Freq)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~Hair) 10.8 External resources Cookbook for R: Discussion on reordering the levels of a factor. DataCamp Exercise: Simple exercise on making bar graphs with ggplot2. ggplot2 cheatsheet: Always good to have close by. "],
["cleveland.html", "11 Chart: Cleveland Dot Plot 11.1 Overview 11.2 Multiple dots", " 11 Chart: Cleveland Dot Plot This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 11.1 Overview This section covers how to make Cleveland dot plots. Cleveland dot plots are a great alternative to a simple bar chart, particularly if you have more than a few items. It doesn’t take much for a bar chart to look cluttered. In the same amount of space, many more values can be included in a dot plot, and it’s easier to read as well. R has a built-in base function, dotchart(), but since it’s such an easy graph to draw, doing it “from scratch” in ggplot2 or base allows for more customization. The code: library(tidyverse) # create a theme for dot plots, which can be reused theme_dotplot &lt;- theme_bw(14) + theme(axis.text.y = element_text(size = rel(.75)), axis.ticks.y = element_blank(), axis.title.x = element_text(size = rel(.75)), panel.grid.major.x = element_blank(), panel.grid.major.y = element_line(size = 0.5), panel.grid.minor.x = element_blank()) # move row names to a dataframe column df &lt;- swiss %&gt;% tibble::rownames_to_column(&quot;Province&quot;) # create the plot ggplot(df, aes(x = Fertility, y = reorder(Province, Fertility))) + geom_point(color = &quot;blue&quot;) + scale_x_continuous(limits = c(35, 95), breaks = seq(40, 90, 10)) + theme_dotplot + xlab(&quot;\\nannual live births per 1,000 women aged 15-44&quot;) + ylab(&quot;French-speaking provinces\\n&quot;) + ggtitle(&quot;Standardized Fertility Measure\\nSwitzerland, 1888&quot;) 11.2 Multiple dots For this example we’ll use 2010 data on SAT mean scores for a sample of New York City public schools: df &lt;- read_csv(&quot;data/SAT2010.csv&quot;, na = &quot;s&quot;) ## Parsed with column specification: ## cols( ## DBN = col_character(), ## `School Name` = col_character(), ## `Number of Test Takers` = col_double(), ## `Critical Reading Mean` = col_double(), ## `Mathematics Mean` = col_double(), ## `Writing Mean` = col_double() ## ) set.seed(5293) tidydf &lt;- df %&gt;% filter(!is.na(`Critical Reading Mean`)) %&gt;% sample_n(20) %&gt;% rename(Reading = &quot;Critical Reading Mean&quot;, Math = &quot;Mathematics Mean&quot;, Writing = &quot;Writing Mean&quot;) %&gt;% gather(key = &quot;Test&quot;, value = &quot;Mean&quot;, &quot;Reading&quot;, &quot;Math&quot;, &quot;Writing&quot;) ggplot(tidydf, aes(Mean, `School Name`, color = Test)) + geom_point() + ggtitle(&quot;Schools are sorted alphabetically&quot;, sub = &quot;not the best option&quot;) + ylab(&quot;&quot;) + theme_dotplot Note that School Name is sorted by factor level, which by default is alphabetical. A better choice is to sort by one of the levels of Test. It’s usually best to try sorting on different factor levels and observe the patterns that appear. To perform the double sort, that is, sorting School Name by Test and then Mean, we use forcats::fct_reorder2(). This function sorts .f (a factor or character vector) by two sorting vectors, .x and .y. For this type of plot, .x is the variable represented by the colored dots and .y is the continuous variable mapped to the y-axis. Suppose we wish to sort the schools by mean reading score. We can do this by limiting the Test variable to “Reading” when sorting on Mean: ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test==&quot;Reading&quot;, Mean, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Reading mean&quot;) + ylab(&quot;&quot;) + theme_dotplot (Many thanks to Zeyu Qiu for the tip on setting .x directly to the factor level, a much better approach than reordering factor levels to conform with fct_reorder2() defaults, as discussed below.) While this is the go-to method, there may be cases in which it’s easier to specify that you wish to sort by the first or the last factor level of the first sorting variable (Test), without spelling it out. If a factor level is not specified, fct_reorder2() by default will sort on the last factor level of .x. In this case, “Writing” is the last factor level of Test: ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test, Mean, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Writing mean&quot;) + ylab(&quot;&quot;) + theme_dotplot If you desire to sort by the first factor level of .x, “Math” in this case, you’ll need the development version of forcats, which you can install with: devtools::install_github(&quot;tidyverse/forcats&quot;) Change the default sorting function, last2(), to first2(): ggplot(tidydf, aes(Mean, fct_reorder2(`School Name`, Test, Mean, .fun = first2, .desc = FALSE), color = Test)) + geom_point() + ggtitle(&quot;Schools sorted by Math mean&quot;) + ylab(&quot;&quot;) + theme_dotplot "],
["scatter.html", "12 Chart: Scatterplot 12.1 Overview 12.2 tl;dr 12.3 Simple examples 12.4 Theory 12.5 When to use 12.6 Considerations 12.7 Modifications 12.8 External resources", " 12 Chart: Scatterplot 12.1 Overview This section covers how to make scatterplots 12.2 tl;dr Fancy Example NOW! Gimme Gimme GIMME! Here’s a look at the relationship between brain weight vs. body weight for 62 species of land mammals: And here’s the code: library(MASS) # data library(ggplot2) # plotting # ratio for color choices ratio &lt;- mammals$brain / (mammals$body*1000) ggplot(mammals, aes(x = body, y = brain)) + # plot points, group by color geom_point(aes(fill = ifelse(ratio &gt;= 0.02, &quot;#0000ff&quot;, ifelse(ratio &gt;= 0.01 &amp; ratio &lt; 0.02, &quot;#00ff00&quot;, ifelse(ratio &gt;= 0.005 &amp; ratio &lt; 0.01, &quot;#00ffff&quot;, ifelse(ratio &gt;= 0.001 &amp; ratio &lt; 0.005, &quot;#ffff00&quot;, &quot;#ffffff&quot;))))), col = &quot;#656565&quot;, alpha = 0.5, size = 4, shape = 21) + # add chosen text annotations geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Mouse&quot;, &quot;Human&quot;, &quot;Asian elephant&quot;, &quot;Chimpanzee&quot;, &quot;Owl monkey&quot;, &quot;Ground squirrel&quot;), paste(as.character(row.names(mammals)), &quot;→&quot;, sep = &quot; &quot;),&#39;&#39;)), hjust = 1.12, vjust = 0.3, col = &quot;grey35&quot;) + geom_text(aes(label = ifelse(row.names(mammals) %in% c(&quot;Golden hamster&quot;, &quot;Kangaroo&quot;, &quot;Water opossum&quot;, &quot;Cow&quot;), paste(&quot;←&quot;, as.character(row.names(mammals)), sep = &quot; &quot;),&#39;&#39;)), hjust = -0.12, vjust = 0.35, col = &quot;grey35&quot;) + # customize legend/color palette scale_fill_manual(name = &quot;Brain Weight, as the\\n% of Body Weight&quot;, values = c(&#39;#d7191c&#39;,&#39;#fdae61&#39;,&#39;#ffffbf&#39;,&#39;#abd9e9&#39;,&#39;#2c7bb6&#39;), breaks = c(&quot;#0000ff&quot;, &quot;#00ff00&quot;, &quot;#00ffff&quot;, &quot;#ffff00&quot;, &quot;#ffffff&quot;), labels = c(&quot;Greater than 2%&quot;, &quot;Between 1%-2%&quot;, &quot;Between 0.5%-1%&quot;, &quot;Between 0.1%-0.5%&quot;, &quot;Less than 0.1%&quot;)) + # formatting scale_x_log10(name = &quot;Body Weight&quot;, breaks = c(0.01, 1, 100, 10000), labels = c(&quot;10 g&quot;, &quot;1 kg&quot;, &quot;100 kg&quot;, &quot;10K kg&quot;)) + scale_y_log10(name = &quot;Brain Weight&quot;, breaks = c(1, 10, 100, 1000), labels = c(&quot;1 g&quot;, &quot;10 g&quot;, &quot;100 g&quot;, &quot;1 kg&quot;)) + ggtitle(&quot;An Elephant Never Forgets...How Big A Brain It Has&quot;, subtitle = &quot;Brain and Body Weights of Sixty-Two Species of Land Mammals&quot;) + labs(caption = &quot;Source: MASS::mammals&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) + theme(legend.position = c(0.832, 0.21)) For more info on this dataset, type ?MASS::mammals into the console. And if you are going crazy not knowing what species is in the top right corner, it’s another elephant. Specifically, it’s the African elephant. It also never forgets how big a brain it has. 12.3 Simple examples That was too fancy! Much simpler please! Let’s use the SpeedSki dataset from GDAdata to look at how the speed achieved by the participants related to their birth year: library(GDAdata) head(SpeedSki, n = 7) ## Rank Bib FIS.Code Name Year Nation Speed Sex Event ## 1 1 61 7039 ORIGONE Simone 1979 ITA 211.67 Male Speed One ## 2 2 59 7078 ORIGONE Ivan 1987 ITA 209.70 Male Speed One ## 3 3 66 190130 MONTES Bastien 1985 FRA 209.69 Male Speed One ## 4 4 57 7178 SCHROTTSHAMMER Klaus 1979 AUT 209.67 Male Speed One ## 5 5 69 510089 MAY Philippe 1970 SUI 209.19 Male Speed One ## 6 6 75 7204 BILLY Louis 1993 FRA 208.33 Male Speed One ## 7 7 67 7053 PERSSON Daniel 1975 SWE 208.03 Male Speed One ## no.of.runs ## 1 4 ## 2 4 ## 3 4 ## 4 4 ## 5 4 ## 6 4 ## 7 4 12.3.1 Scatterplot using base R x &lt;- SpeedSki$Year y &lt;- SpeedSki$Speed # plot data plot(x, y, main = &quot;Scatterplot of Speed vs. Birth Year&quot;) Base R scatterplots are easy to make. All you need are the two variables you want to plot. Although scatterplots can be made with categorical data, the variables you are plotting will usually be continuous. 12.3.2 Scatterplot using ggplot2 library(GDAdata) # data library(ggplot2) # plotting # main plot scatter &lt;- ggplot(SpeedSki, aes(Year, Speed)) + geom_point() # show with trimmings scatter + labs(x = &quot;Birth Year&quot;, y = &quot;Speed Achieved (km/hr)&quot;) + ggtitle(&quot;Ninety-One Skiers by Birth Year and Speed Achieved&quot;) ggplot2 makes it very easy to create scatterplots. Using geom_point(), you can easily plot two different aesthetics in one graph. It also is simple to add on extra formatting to make your plots look nice (All that is really necessary is the data, the aesthetics, and the geom). 12.4 Theory Scatterplots are very useful in understanding the correlation (or lack thereof) between variables. For example, in section 13.2 notice the positive relationship between brain and body weight in species of land mammals. The scatterplot gives a good idea of whether that relationship is positive or negative and if there’s a correlation. However, don’t mistake correlation in a scatterplot for causation! Below we show variations on the scatterplot which can be used to enhance interpretability. For more info about adding lines/contours, comparing groups, and plotting continuous variables check out Chapter 5 of the textbook. 12.5 When to use Scatterplots are great for exploring relationships between variables. Basically, if you are interested in how variables relate to each other, the scatterplot is a great place to start. 12.6 Considerations 12.6.1 Overlapping data Data with similar values will overlap in a scatterplot and may lead to problems. Consider exploring alpha blending or jittering as remedies (links from Overlapping Data section of Iris Walkthrough). 12.6.2 Scaling Consider how scaling can modify how your data will be perceived: library(ggplot2) num_points &lt;- 100 wide_x &lt;- c(rnorm(n = 50, mean = 100, sd = 2), rnorm(n = 50, mean = 10, sd = 2)) wide_y &lt;- rnorm(n = num_points, mean = 5, sd = 2) df &lt;- data.frame(wide_x, wide_y) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Linear X-Axis&quot;) ggplot(df, aes(wide_x, wide_y)) + geom_point() + ggtitle(&quot;Log-10 X-Axis&quot;) + scale_x_log10() 12.7 Modifications 12.7.1 Contour lines Contour lines give a sense of the density of the data at a glance. For these contour maps, we will use the SpeedSki dataset. Contour lines can be added to the plot call using geom_density_2d(): ggplot(SpeedSki, aes(Year, Speed)) + geom_density_2d() Contour lines work best when combined with other layers: ggplot(SpeedSki, aes(Year, Speed)) + geom_point() + geom_density_2d(bins = 5) 12.7.2 Scatterplot matrices If you want to compare multiple parameters to each other, consider using a scatterplot matrix. This will allow you to show many comparisons in a compact and efficient manner. For these scatterplot matrices, we will use the movies dataset from the ggplot2movies package. As a default, the base R plot() function will create a scatterplot matrix when given multiple variables: library(ggplot2movies) # data library(dplyr) # manipulation index &lt;- sample(nrow(movies), 500) #sample data moviedf &lt;- movies[index,] # data frame splomvar &lt;- moviedf %&gt;% dplyr::select(length, budget, votes, rating, year) plot(splomvar) While this is quite useful for personal exploration of a datset, it is not recommended for presentation purposes. Something called the Hermann grid illusion makes this plot very difficult to examine. To remove this problem, consider using the splom() function from the lattice package: library(lattice) #sploms splom(splomvar) 12.8 External resources Quick-R article about scatterplots using Base R. Goes from the simple into the very fancy, with Matrices, High Density, and 3D versions. STHDA Base R: article on scatterplots in Base R. More examples of how to enhance the humble graph. STHDA ggplot2: article on scatterplots in ggplot2. Heavy on the formatting options available and facet warps. Stack Overflow on adding labels to points from geom_point() ggplot2 cheatsheet: Always good to have close by. "],
["iris.html", "13 Walkthrough: Iris Scatterplot 13.1 Overview 13.2 Quick note on doing it the lazy way 13.3 Viewing data 13.4 Plotting data 13.5 Markdown etiquette 13.6 Overlapping data 13.7 Formatting for presentation 13.8 Alter appearance 13.9 Consider themes 13.10 Going deeper 13.11 Helpful links", " 13 Walkthrough: Iris Scatterplot 13.1 Overview This example goes through some work with the iris dataset to get to a finished scatterplot that is ready to present. 13.1.1 tl;dr Here’s what we end up with: library(ggplot2) base_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) base_plot + theme_minimal() Wondering how we got there? Read on. 13.1.2 Packages ggplot2 dplyr stats Base datasets (gridExtra) 13.1.3 Techniques Keyboard Shortcuts Viewing Data Structure/Dimensions/etc. Accessing Documentation Plotting with ggplot2 Layered Nature of ggplot2/Grammar of Graphics Mapping aesthetics in ggplot2 Overlapping Data: alpha and jitter Presenting Graphics Themes 13.2 Quick note on doing it the lazy way Shortcuts are your best friend to get work done faster. And they are easy to find. In the toolbar: Tools &gt; Keyboard Shortcuts Help OR ⌥⇧K Some good ones: Insert assignment operator (&lt;-): Alt/Option+- Insert pipe (%&gt;%): Ctrl/Cmd+Shift+M Comment Code: Ctrl/Cmd+Shift+C Run current line/selection: Ctrl/Cmd+Enter Re-run previous region: Ctrl/Cmd+Shift+P Be on the lookout for things you do often and try to see if there is a faster way to do them. Additionally, the RStudio IDE can be a little daunting, but it is full of useful tools that you can read about in this cheatsheet or go through with this DataCamp course: Part 1, Part 2. Okay, now let’s get to it… 13.3 Viewing data Let’s start with loading the package so we can get the data as a dataframe. library(datasets) class(iris) ## [1] &quot;data.frame&quot; This is not a huge dataset, but it is helpful to get into the habit of treating datasets as large no matter what. Because of this, make sure you inspect the size and structure of your dataset before going and printing it to the console. Here we can see that we have 150 observations across 5 different variables. dim(iris) ## [1] 150 5 There are a bunch of ways to get information on your dataset. Here are a few: str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## # This one requires dplyr, but it&#39;s worth it :) library(dplyr) glimpse(iris) ## Observations: 150 ## Variables: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, set… Plotting the data by calling iris to the console will print the whole thing. Go ahead and try it in this case, but this is not recommended for larger datasets. Instead, use head() in the console or View(). If you want to learn more about these commands, or anything for that matter, just type ?&lt;command&gt; into the console. ?head, for example, will reveal that there is an additional argument to head called n for the number of lines printed, which defaults to 6. Also, you may notice there is something called tail. I wonder what that does? 13.4 Plotting data Let’s plot something! # Something&#39;s missing library(ggplot2) ggplot(iris) Where is it? Maybe if we add some aesthetics. I remember that was an important word that came up somewhere: # Still not working... ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) Still nothing. Remember, you have to add a geom for something to show up. # There we go! ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() Yay! Something showed up! Notice where we put the data, inside of ggplot(). ggplot is built on layers. Here we put it in the main call to ggplot. The data argument is also available in geom_point(), but in that case it would only apply to that layer. Here, we are saying, for all layers, unless specified, make the data be iris. Now let’s add a color mapping by Species: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species)) Usually it is helpful to store the main portion of the plot in a variable and add on the layers. The code below achieves the same output as above: sepal_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) sepal_plot + geom_point(aes(color = Species)) 13.5 Markdown etiquette I’m seeing that my R Markdown file is getting a little messy. Working with markdown and chunks can get out of hand, but there are some helpful tricks. First, consider naming your chunks as you go. If you combine this with headers, your work will be much more organized. Specifically, the little line at the bottom of the editor becomes much more useful. From this: To this: Just add a name to the start of each chunk: {r &lt;cool-code-chunk-name&gt;, &lt;chunk_option&gt; = TRUE} Now you can see what the chunks were about as well as get a sense of where you are in the document. Just don’t forget, it is a space after the r and commas for the other chunk options you may have like eval or echo. For more info, see our section on communicating results. 13.6 Overlapping data Eagle-eyed viewers may notice that we seem to be a few points short. We should be seeing 150 points, but we only see 117 (yes, I counted). Where are those 33 missing points? They are actually hiding behind other points. This dataset rounds to the nearest tenth of a centimeter, which is what is giving us those regular placings of the points. How did I know the data was in centimeters? Running ?iris in the console of course! Ah, you ask a silly question, you get a silly answer. # This plot hides some of the points ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species)) What’s the culprit? The color aesthetic. The color by default is opaque and will hide any points that are behind it. As a rule, it is always beneficial to reduce the opacity a little no matter what to avoid this problem. To do this, change the alpha value to something other than it’s default 1, like 0.5. ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species, alpha = 0.5)) Okay…a couple things with this. 13.6.1 First: the legend First, did you notice the new addition to the legend? That looks silly! Why did that show up? Well, when we added the alpha into aes(), we got a new legend. Let’s look at what we are doing with geom_point(). Specifically, this is saying how we should map the color and alpha: geom_point(mapping = aes(color = Species, alpha = 0.5)) So, we are mapping these given aesthetics, color and alpha, to certain values. ggplot knows that usually the aesthetic mapping will vary since you are probably passing in data that varies, so it will create a legend for each mapping. However, we don’t need a legend for the alpha: we explicitly set it to be 0.5. To fix this, we can pull alpha out of aes and instead treat it like an attribute: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5) No more legend. So, in ggplot, there is a difference between where an aesthetic is placed. It is also called MAPPING an aesthetic (making it vary with data inside aes) or SETTING an aesthetic (make it a constant attribute across all datapoints outside of aes). 13.6.2 Second: jittering Secondly, did this alpha trick really help us? Are we able to see anything in the plot in an easier way? Not really. Since the points perfectly overlap, the opacity difference doesn’t help us much. Usually, opacity will work, but here the data is so regular that we don’t gain anything in the perception department. We can fix this by introducing some jitter to the datapoints. Jitter adds a little random noise and moves the datapoints so that they don’t fully overlap: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) Consider your motives when using jittering. You are by definition altering the data, but it may be beneficial in some situations. 13.6.3 Aside: example where alpha blending works We are dealing with a case where jittering works best to see the data, while changing the alpha doesn’t help us much. Here’s a quick example where opacity using alpha might be more directly helpful. # lib for arranging plots side by side library(gridExtra) # make some normally distributed data x_points &lt;- rnorm(n = 10000, mean = 0, sd = 2) y_points &lt;- rnorm(n = 10000, mean = 6, sd = 2) df &lt;- data.frame(x_points, y_points) # plot with/without changed alpha plt1 &lt;- ggplot(df, aes(x_points, y_points)) + geom_point() + ggtitle(&quot;Before (alpha = 1)&quot;) plt2 &lt;- ggplot(df, aes(x_points, y_points)) + geom_point(alpha = 0.1) + ggtitle(&quot;After (alpha = 0.1)&quot;) # arrange plots gridExtra::grid.arrange(plt1, plt2, ncol = 2, nrow = 1) Here it is much easier to see where the dataset is concentrated. 13.7 Formatting for presentation Let’s say we have finished this plot and we are ready to present it to other people: We should clean it up a bit so it can stand on its own. 13.8 Alter appearance First, let’s make the x/y labels a little cleaner and more descriptive: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) Next, add a title that encapsulates the plot: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) And make the points a little bigger: ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) Now it’s looking presentable. 13.9 Consider themes It may be better for your situation to change the theme of the plot (the background, axes, etc.; the “accessories” of the plot). Explore what different themes can offer and pick one that is right for you. base_plot &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point(aes(color = Species), size = 3, alpha = 0.5, position = &quot;jitter&quot;) + xlab(&quot;Sepal Length (cm)&quot;) + ylab(&quot;Sepal Width (cm)&quot;) + ggtitle(&quot;Sepal Dimensions in Different Species of Iris Flowers&quot;) base_plot base_plot + theme_light() base_plot + theme_minimal() base_plot + theme_classic() base_plot + theme_void() I’m going to go with theme_minimal() this time. So here we are! We got a lovely scatterplot ready to show the world! 13.10 Going deeper We have just touched the surface of ggplot and dipped our toes into grammar of graphics. If you want to go deeper, I highly recommend the DataCamp courses on Data Visualization with ggplot2 with Rick Scavetta. There are three parts and they are quite dense, but the first part is definitely worth checking out. 13.11 Helpful links RStudio ggplot2 Cheat Sheet DataCamp: Mapping aesthetics to things in ggplot R Markdown Reference Guide R for Data Science "],
["parallelcoordinates.html", "14 Chart: Parallel Coordinate Plots 14.1 Overview 14.2 tl;dr 14.3 Simple examples 14.4 Theory 14.5 When to use 14.6 Considerations 14.7 Modifications 14.8 Other Packages 14.9 External Resources", " 14 Chart: Parallel Coordinate Plots This chapter originated as a community contribution created by aashnakanuga 14.1 Overview This section covers how to create static parallel coordinate plots with the GGally package. For interactive parallel coordinate plots, check out the parcoords package. The package vignette provides instructions on using this package. 14.2 tl;dr I want a Fancy Example! Not tomorrow, not after breakfast, NOW! Here’s a look at the effect of different attributes on each Fair cut diamond from the “diamonds” dataset: And here’s the code: library(GGally) library(dplyr) #subset the data to get the first thousand cases diamonds_subset &lt;- subset(diamonds[1:1000,]) #rename the variables to understand what they signify names(diamonds_subset)&lt;-c(&quot;carat&quot;,&quot;cut&quot;,&quot;color&quot;,&quot;clarity&quot;,&quot;depth_percentage&quot;,&quot;table&quot;,&quot;price&quot;,&quot;length&quot;,&quot;width&quot;,&quot;depth&quot;) #Create a new column to highlight the fair cut diamonds ds_fair&lt;-within(diamonds_subset, diamond_cut&lt;-if_else(cut==&quot;Fair&quot;, &quot;Fair&quot;, &quot;Other&quot;)) #Create the graph ggparcoord(ds_fair[order(ds_fair$diamond_cut, decreasing=TRUE),], columns=c(1,5,7:10), groupColumn = &quot;diamond_cut&quot;, alphaLines = 0.8, order=c(5,1,8,9,10,7), title = &quot;Parallel Coordinate Plot showing trends for Fair cut diamonds&quot;, scale = &quot;uniminmax&quot;) + scale_color_manual(values=c(&quot;maroon&quot;,&quot;gray&quot;)) For more information about the dataset, type ?diamonds into the console. 14.3 Simple examples Woah woah woah! Too complicated! Much simpler, please. Let us use the popular “iris” dataset for this example: library(datasets) library(GGally) ggparcoord(iris, columns=1:4, title = &quot;Parallel coordinate plot for Iris flowers&quot;) For more information about the dataset, type ?iris into the console. 14.4 Theory For more info about parallel coordinate plots and multivariate continuous data, check out Chapter 6 of the textbook. 14.5 When to use Generally, parallel coordinate plots are used to infer relationships between multiple continuous variables - we mostly use them to detect a general trend that our data follows, and also the specific cases that are outliers. Please keep in mind that parallel coordinate plots are not the ideal graph to use when there are just categorical variables involved. We can include a few categorical variables in our axes or for the sake of clustering, but using a lot of categorical variables results in overlapping profiles, which makes it difficult to interpret. We can also use parallel coordinate plots to identify trends in specific clusters - just highlight each cluster in a different color using the groupColumn attribute of ggparcoord() to specify your column, and you are good to go! Sometimes, parallel coordinate plots are very helpful in graphing time series data - where we have information stored at regular time intervals. Each vertical axis will now become a time point and we need to pass that column in ggparcoord’s “column” attribute. 14.6 Considerations 14.6.1 When do I use clustering? Generally, you use clustering when you want to observe a pattern in a set of cases with some specific properties. This may include divvying up all variables into clusters based on their value for a specific categorical variable. But you can even use a continuous variable; for example, dividing all cases into two sections based on some continuous variable height: those who have a height greater than 150cm and those who do not. Let us look at an example using our iris dataset, clustering on the “Species” column: library(GGally) #highlight the clusters based on the Species column graph&lt;-ggparcoord(iris, columns=1:4, groupColumn = 5, title = &quot;Plot for Iris data, where each color represents a specific Species&quot;) graph 14.6.2 Deciding the value of alpha In practice, parallel coordinate plots are not going to be used for very small datasets. Your data will likely have thousands and thousands of cases, and sometimes it can get very difficult to observe anything when so many of your cases will overlap. So we set the aplhaLines parameter to a value between zero and one, and it reduces the opacity of all lines so that you can get a clearer view of what is going on if you have too many overlapping cases. Again we use our iris data, but reduce alpha to 0.5. Observe how much easier it is now to trace the course of every case: library(ggplot2) library(GGally) #set the value of alpha to 0.5 ggparcoord(iris, columns=1:4, groupColumn = 5, alphaLines = 0.5, title = &quot;Iris data with a lower alpha value&quot;) 14.6.3 Scales When we use ggparcoord(), we have an option to set the scale attribute, which will scale all variables so we can compare their values. The different types of scales are as follows: std: default value, where it subtracts mean and divides by SD robust: subtract median and divide by median absolute deviation uniminmax: scale all values so that the minimum is at 0 and maximum at 1 globalminmax: no scaling, original values taken center: centers each variable according to the value given in scaleSummary centerObs: centers each variable according to the value of the observation given in centerObsID Let us create a sample dataset and see how values on the y-axis change for different scales: library(ggplot2) library(GGally) library(gridExtra) #creating a sample dataset df1&lt;-data.frame(col1=c(11,4,7,4,3,8,5,7,9), col2=c(105,94,138,194,173,129,156,163,148)) #pay attention to the different values on the y-axis g1&lt;-ggparcoord(df1, columns=1:2, scale = &quot;std&quot;, title = &quot;Standard Scale&quot;) g2&lt;-ggparcoord(df1, columns=1:2, scale = &quot;robust&quot;, title = &quot;Robust Scale&quot;) g3&lt;-ggparcoord(df1, columns=1:2, scale = &quot;uniminmax&quot;, title = &quot;Uniminmax Scale&quot;) g4&lt;-ggparcoord(df1, columns=1:2, scale = &quot;globalminmax&quot;, title = &quot;Globalminmax Scale&quot;) g5&lt;-ggparcoord(df1, columns=1:2, scale = &quot;center&quot;, scaleSummary = &quot;mean&quot;, title = &quot;Center Scale&quot;) g6&lt;-ggparcoord(df1, columns=1:2, scale = &quot;centerObs&quot;, centerObsID = 4, title = &quot;CenterObs Scale&quot;) grid.arrange(g1, g2, g3, g4, g5, g6, nrow=2) 14.6.4 Order of the variables Deciding the order of the variables on the y-axis depends on your application. It can be specified using the order parameter. The different types of order are as follows: default: the order in which we add our variables to the column attribute given vector: providing a vector of the order we need (used most frequently) anyClass: order based on the separation of a variable from the rest (F-statistic - each variable v/s the rest) allClass: order based on the variation between classes (F-statistic - group column v/s the rest) skewness: order from most to least skewed Outlying: order based on the Outlying measure 14.7 Modifications 14.7.1 Flipping the coordinates A good idea if we have too many variables and their names are overlapping on the x-axis: library(ggplot2) library(GGally) #using the iris dataset graph + coord_flip() 14.7.2 Highlighting trends Let us see what trend the versicolor Species of the iris dataset follows over the other variables: library(ggplot2) library(GGally) library(dplyr) #get a new column that says &quot;Yes&quot; of the Species is versicolor. ds_versi&lt;-within(iris, versicolor&lt;-if_else(Species==&quot;versicolor&quot;, &quot;Yes&quot;, &quot;No&quot;)) ggparcoord(ds_versi[order(ds_versi$versicolor),], columns = 1:4, groupColumn = &quot;versicolor&quot;, title = &quot;Highlighting trends of Versicolor species&quot;) + scale_color_manual(values=c(&quot;gray&quot;,&quot;maroon&quot;)) 14.7.3 Using splines Generally, we use splines if we have a column where there are a lot of repeating values, which adds a lot of noise. The case lines become more and more curved when we set a higher spline factor, which removes noise and makes for easier observations of trends. It can be set using the splineFactor attribute: library(ggplot2) library(GGally) library(gridExtra) #create a sample dataset df2&lt;-data.frame(col1=c(1:9), col2=c(11,11,14,15,15,15,17,18,18), col3=c(4,4,4,7,7,7,8,9,9), col4=c(3,3,3,4,6,6,6,8,8)) #plot without spline g7&lt;-ggparcoord(df2, columns = 1:4, scale = &quot;globalminmax&quot;, title = &quot;No Spline factor&quot;) #plot with spline g8&lt;-ggparcoord(df2, columns = 1:4, scale = &quot;globalminmax&quot;, splineFactor=10, title = &quot;Spline factor set to 10&quot;) grid.arrange(g7,g8) 14.7.4 Adding boxplots to the graph You can add boxplots to your graph, which can be useful for observing the trend of median values. Generally, they are added to data with a lot of variables - for example, if we plot time series data. 14.8 Other Packages There are a number of packages that have functions for creating parallel coordinate plots: [to do: add links] parcoords::parcoords() – great interactive option ggplot2::geom_line() – not specific to parallel coordinate plots but easy to create with the group= parameter. lattice::parallelplot() MASS::parcoord() 14.9 External Resources Introduction to parallel coordinate plots: An excellent resource giving details of all attributes and possible values. Also has some good examples. How to create interactive parallel coordinate plots: a nice walkthrough on using plotly to create an interactive parallel coordinate plot. Different methods to create parallel coordinate plots: This is specifically when we have categorical variables. "],
["mosaic.html", "15 Chart: Mosaic 15.1 Overview 15.2 tl;dr 15.3 Simple Example Walkthrough 15.4 Mosaic using ggplot 15.5 Theory 15.6 When to use 15.7 Considerations 15.8 External resources", " 15 Chart: Mosaic This chapter originated as a community contribution created by harin This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 15.1 Overview This section covers how to make Mosaic plots 15.2 tl;dr library(vcd) mosaic(Favorite ~ Age + Music, labeling = labeling_border( abbreviate_labs = c(3, 10, 6), rot_labels=c(0,0,-45) ), direction=c(&#39;v&#39;,&#39;v&#39;,&#39;h&#39;), # Age = Vertical, Music = Vertical, Favoriate = Horizonal (a.k.a DoubleDecker) gp = gpar(fill=c(&#39;lightblue&#39;, &#39;gray&#39;)), df) 15.3 Simple Example Walkthrough 15.3.1 Order of splits It is best to draw mosaic plots incrementally: start with splitting on one variable and then add additional variables one at a time. The full mosaic plot will have one split per variable. Important: if your data is in a data frame, as in the example below, the count column must be called Freq. (Tables and matrices also work, see ?vcd::structable for more details.) Also note that all of these plots are drawn with vcd::mosaic() not the base R function, mosaicplot(). The data: df ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 ## 7 young coffee classical 1 ## 8 young coffee rock 0 Split on Age only: library(vcd) mosaic(~Age, df) Split on Age, then Music: mosaic(Music ~ Age, df) Note that the first split is between “young” and “old”, while the second set of splits divides each age group into “classical” and “rock”. Split on Age, then Music, then Favorite: mosaic(Favorite ~ Age + Music, df) 15.3.2 Direction of splits Note that in the previous example, the direction of the splits is as follows: Age – horizontal split Music – vertical split Favorite – horizontal split This is the default direction pattern: alternating directions beginning with horizontal. Therefore we get the same plot with the following: mosaic(Favorite ~ Age + Music, direction = c(&quot;h&quot;, &quot;v&quot;, &quot;h&quot;), df) The directions can be altered as desired. For example, to create a doubledecker plot, make all splits vertical except the last one: mosaic(Favorite ~ Age + Music, direction = c(&quot;v&quot;, &quot;v&quot;, &quot;h&quot;), df) Note that the direction vector is in order of splits (Age, Music, Favorite), not in the order in which the variables appear in the formula, where the last variable to be split is listed first, before the “~”. 15.3.3 Options 15.3.3.1 Fill color: library(grid) # needed for gpar mosaic(Favorite ~ Age + Music, gp = gpar(fill = c(&quot;lightblue&quot;, &quot;blue&quot;)), df) 15.3.3.2 Rotate labels: mosaic(Favorite ~ Age + Music, labeling = labeling_border(rot_labels = c(45, -45, 0, 0)), df) The rot_labels = vector sets the rotation in degrees on the four sides of the plot in this order: top, right, bottom, left. (Different from the typical base graphics order!) The default is rot_labels = c(0, 90, 0, 90). 15.3.3.3 Abbreviate labels: mosaic(Favorite ~ Age + Music, labeling = labeling_border(abbreviate_labs = c(3, 1, 6)), df) Labels are abbreviated in the order of the splits (as for direction =). The abbreviation algorithm appears to return the specified number of characters after vowels are eliminated (if necessary). For more formatting options, see &gt;?vcd::labeling_border. 15.3.3.4 Remove spacing between cells mosaic(Favorite ~ Age + Music, spacing = spacing_equal(sp = unit(0, &quot;lines&quot;)), df) For more details, see &gt;?vcd::spacings 15.3.3.5 Change border color (must also set fill(?)) mosaic(Favorite ~ Age + Music, gp = gpar(fill = c(&quot;lightblue&quot;, &quot;blue&quot;), col = &quot;white&quot;), spacing = spacing_equal(sp = unit(0, &quot;lines&quot;)), df) 15.3.4 Mosaic using vcd::doubledecker data(Arthritis) vcd::doubledecker(Improved ~ Treatment + Sex, data=Arthritis) vcd::doubledecker(Music ~ Favorite + Age, xtabs(Freq ~ Age + Music + Favorite, df)) 15.4 Mosaic using ggplot For a comprehensive overview of mosaic plot in ggplot check out the link below. https://cran.r-project.org/web/packages/ggmosaic/vignettes/ggmosaic.html library(ggmosaic) # equivalent to doing Favorite ~ Age + Music in vcd::mosaic with doubledecker style cut ggplot(df) + geom_mosaic( aes(x=product(Favorite, Age, Music), # cut from right to left weight=Freq, fill=Favorite ), divider=c(&quot;vspine&quot; , &quot;hspine&quot;, &quot;hspine&quot;) # equivalent to divider=ddecker() ) 15.5 Theory 15.6 When to use When you want to see the relationships in Multivariate Categorical Data 15.7 Considerations 15.7.1 Labels Legibility of the labels is problematic in mosaic plot especially when there are a lot of dimensions. This can be alleviated by - Abbreviate names - Rotating the labels 15.7.2 Aspect Ratio lengths are easier to judge than area, so try to use rectangles with same width or height Taller thinner rectangles are better (we are better at distinguishing length than area) 15.7.3 Gaps between rectangles No gap = most efficient However, a gap can help improve legibility, so try out different combinations Can have a gap at splits Can Vary gap size down the hierarchy 15.7.4 Color good for rates in the subgroup displaying residual emphasizing particular subgroup 15.8 External resources Chapter 7 of Graphical data analysis with R by Anthony Unwin Link: A comprehensive overview of mosaic plot in ggplot check out the link below. "],
["heatmap.html", "16 Chart: Heatmap 16.1 Overview 16.2 tl;dr 16.3 Simple examples 16.4 Theory 16.5 External resources", " 16 Chart: Heatmap 16.1 Overview This section covers how to make heatmaps. 16.2 tl;dr Enough with these simple examples! I want a complicated one! Here’s a heatmap of occupational categories of sons and fathers in the US, UK, and Japan: And here’s the code: library(vcdExtra) # dataset library(dplyr) # manipulation library(ggplot2) # plotting library(viridis) # color palette # format data orderedclasses &lt;- c(&quot;Farm&quot;, &quot;LoM&quot;, &quot;UpM&quot;, &quot;LoNM&quot;, &quot;UpNM&quot;) mydata &lt;- Yamaguchi87 mydata$Son &lt;- factor(mydata$Son, levels = orderedclasses) mydata$Father &lt;- factor(mydata$Father, levels = orderedclasses) japan &lt;- mydata %&gt;% filter(Country == &quot;Japan&quot;) uk &lt;- mydata %&gt;% filter(Country == &quot;UK&quot;) us &lt;- mydata %&gt;% filter(Country == &quot;US&quot;) # convert to % of country and class total mydata_new &lt;- mydata %&gt;% group_by(Country, Father) %&gt;% mutate(Total = sum(Freq)) %&gt;% ungroup() # make custom theme theme_heat &lt;- theme_classic() + theme(axis.line = element_blank(), axis.ticks = element_blank()) # basic plot plot &lt;- ggplot(mydata_new, aes(x = Father, y = Son)) + geom_tile(aes(fill = Freq/Total), color = &quot;white&quot;) + coord_fixed() + facet_wrap(~Country) + theme_heat # plot with text overlay and viridis color palette plot + geom_text(aes(label = round(Freq/Total, 1)), color = &quot;white&quot;) + scale_fill_viridis() + # formatting ggtitle(&quot;Like Father, Like Son&quot;, subtitle = &quot;Heatmaps of occupational categories for fathers and sons, by country&quot;) + labs(caption = &quot;Source: vcdExtra::Yamaguchi87&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;)) + theme(plot.subtitle = element_text(face = &quot;bold&quot;, color = &quot;grey35&quot;)) + theme(plot.caption = element_text(color = &quot;grey68&quot;)) For more info on this dataset, type ?vcdExtra::Yamaguchi87 into the console. 16.3 Simple examples Too complicated! Simplify, man! 16.3.1 Heatmap of two-dimensional bin counts For this heatmap, we will use the SpeedSki dataset. Only two variables, x and y are needed for two-dimensional bin count heatmaps. The third variable–i.e., the color–represents the bin count of points in the region it covers. Think of it as a two-dimensional histogram. To create a heatmap, simply substitute geom_point() with geom_bin2d(): library(ggplot2) # plotting library(GDAdata) # data (SpeedSki) ggplot(SpeedSki, aes(Year, Speed)) + geom_bin2d() 16.3.2 Heat map of dataframe To get a visual sense of the dataframe, you can use a heatmap. You can also look into scaling the columns to get a sense of your data on a common scale. In this example, we use geom_tile to graph all cells in the dataframe and color them by their value: library(pgmm) # data library(tidyverse) # processing/graphing library(viridis) # color palette data(wine) # convert to column, value wine_new &lt;- wine %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_new, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe&quot;) # only difference from above is scaling wine_scaled &lt;- data.frame(scale(wine)) %&gt;% rownames_to_column() %&gt;% gather(colname, value, -rowname) ggplot(wine_scaled, aes(x = rowname, y = colname, fill = value)) + geom_tile() + scale_fill_viridis() + ggtitle(&quot;Italian Wine Dataframe, Scaled&quot;) 16.3.3 Modifications You can change the color palette by specifying it explicitly in your chain of ggplot function calls. The bin width can be added inside the geom_bin2d() function call: library(viridis) # viridis color palette # create plot g1 &lt;- ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_viridis() # modify color # show plot g1 + geom_bin2d(binwidth = c(5, 5)) # modify bin width Here are some other examples: # larger bin width g1 + geom_bin2d(binwidth = c(10, 10)) # hexagonal bins g1 + geom_hex(binwidth = c(5, 5)) # hexagonal bins + scatterplot layer g1 + geom_hex(binwidth = c(5, 5), alpha = .4) + geom_point(size = 2, alpha = 0.8) # hexagonal bins with custom color gradient/bin count ggplot(SpeedSki, aes(Year, Speed)) + scale_fill_gradient(low = &quot;#cccccc&quot;, high = &quot;#09005F&quot;) + # color geom_hex(bins = 10) # number of bins horizontally/vertically 16.4 Theory Heat maps are like a combination of scatterplots and histograms: they allow you to compare different parameters while also seeing their relative distributions. While heatmaps are visually striking, there are often better choices to get your point across. For more info, checkout this DataCamp section on heatmaps and alternatives. 16.5 External resources R Graph Gallery: Heatmaps: Has examples of creating heatmaps with the heatmap() function. How to make a simple heatmap in ggplot2: Create a heatmap with geom_tile(). "],
["maps.html", "17 Spatial Data 17.1 Choropleth maps 17.2 Square bins 17.3 Longitude / Latitude data", " 17 Spatial Data This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 17.1 Choropleth maps Choropleth maps use color to indicate the value of a variable within a defined region, generally political boundaries. “Mapping in R just got a whole lot easier” by Sharon Machlis (2017-03-03) offers a tutorial on using the tmap, tmaptools, and tigris packages to create choropleth maps. Note that with this approach, you will need to merge geographic shape files with your data files, and then map. “Step-by-Step Choropleth Map in R: A case of mapping Nepal” walks through the process of creating a choropleth map using rgdal and ggplot2. (We have not followed either of these tutorials step-by-step… if you do, please provide feedback by submitting an issue). The choroplethr package makes it simple to draw choropleth maps of U.S. states, countries, and census tracts, as well as countries of the world without dealing directly with shape files. The companion package, choroplethrZip, provides data for zip code level choropleths; choroplethrAdmin1 draws choropleths for administrative regions of world countries. The following is a brief tutorial on using these packages. Note: You must install also install choroplethrMaps for choroplethr to work. In addition, choroplethr requires a number of other dependencies which should be installed automatically, but if they aren’t, you can manually install the missing packages that you are notified about when you call library(choroplethr): maptools, and rgdal, sp. We’ll use the state.x77 dataset for this example: library(tidyverse) library(choroplethr) # data frame must contain &quot;region&quot; and &quot;value&quot; columns df_illiteracy &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% transmute(region = tolower(`state`), value = Illiteracy) state_choropleth(df_illiteracy, title = &quot;State Illiteracy Rates, 1977&quot;, legend = &quot;Percent Illiterate&quot;) Note: the choroplethr “free course” that you may come across arrives one lesson at a time by email over an extended period so not the best option unless you have a few weeks to spare. 17.2 Square bins Packages such as statebins create choropleth style maps with equal size regions that roughly represent the location of the region, but not the size or shape. Important: Don’t install statebins from CRAN; use the dev version – it contains many improvements, which are detailed in “Statebins Reimagined”. # devtools::install_github(&quot;hrbrmstr/statebins&quot;) library(statebins) df_illit &lt;- state.x77 %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;state&quot;) %&gt;% select(state, Illiteracy) # Note: direction = 1 switches the order of the fill scale # so darker shades represent higher illiteracy rates # (The default is -1). statebins(df_illit, value_col=&quot;Illiteracy&quot;, name = &quot;%&quot;, direction = 1) + ggtitle(&quot;State Illiteracy Rates, 1977&quot;) + theme_statebins() 17.3 Longitude / Latitude data Note that the options above work with political boundaries, based on the names of the regions that you provide. Such maps require packages with geographical boundary information. Longitude / latitude data, on the other hand, can be plotted simply as a scatterplot with x = longitude and y = latitude, without any background maps (just don’t mix up x &amp; y!) The first part of “Data wrangling visualisation and spatial analysis: R Workshop” by C. Brown, D. Schoeman, A. Richardson, and B. Venables provides a detailed walkthrough of spatial exploratory data analysis with copepod data (a type of zooplankton) using this technique with ggplot2::geom_point(). If background maps are desired, there are many options. The tutorial mentioned above provides examples using the maps or sf packages. It is a highly recommended resource as it covers much of the data science pipeline from the context of the problem to obtaining data, cleaning and transforming it, exploring the data, and finally modeling and predicting. Another good choice for background maps is ggmap, which offers several different map source options. Google Maps API was the go-to, but they now require you to enable billing through Google Cloud Platorm. You get $300 in free credit, but if providing a credit card isn’t your thing, you may consider using Stamen Maps instead, with the get_stamenmap() function. Use the development version of the package; instructions and extensive examples are available on the package’s GitHub page “Getting started Stamen maps with ggmap” will help you get started with Stamen maps through an example using the Sacramento dataset in the caret package. "],
["leaflet.html", "18 Interactive Geographic Data 18.1 Overview 18.2 Brief Description about Dataset 18.3 Plotting Markers 18.4 Dynamic Heatmaps 18.5 Dynamic Clustering 18.6 Plotting Groups 18.7 Plotting Categorical Data 18.8 External Resources", " 18 Interactive Geographic Data This chapter originated as a community contribution created by AkhilPunia This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 18.1 Overview You would have already seen different libraries that can help one in beautifully displaying geographic data like ggmap and choroplethr. Even though these libraries provide lots of interesting features to better express information through 2-dimensional graphs, they still lack one feature: interactivity. Here comes leaflet—a library written in javascript to handle interactive maps. Fun Fact: It’s actively used by a lot of leading newspapers like The New York Times and The Washington Post. Let’s dive in. 18.2 Brief Description about Dataset For our analysis, we are using NYC Open Data about schools in New York City in 2016. You can find more about it on the Kaggle page. We will be focusing on the the distribution of different variables as a factor of geographical positions. library(tidyverse) library(leaflet) library(htmltools) library(leaflet.extras) library(viridis) schools &lt;- read_csv(&#39;data/2016_school_explorer.csv&#39;) 18.3 Plotting Markers Here we can see that all the Private schools in NYC have been plotted on a map that allows one to zoom in and out. The markers are used to denote the location of each individual school. If we hover over a marker, it displays the name of the school. Isn’t that cool! Here’s the code for it: lat&lt;-median(schools$Latitude) lon&lt;-median(schools$Longitude) schools %&gt;% filter(`Community School?`==&quot;Yes&quot;) %&gt;% leaflet(options = leafletOptions(dragging = TRUE)) %&gt;% addTiles() %&gt;% addMarkers(label=~`School Name`) %&gt;% setView(lng=lon,lat=lat, zoom = 10) 18.4 Dynamic Heatmaps Heatmaps are really useful tools for visualizing the distribution of a particular variable over a certain region (they are so useful, we got a page on ’em). In this example, we see how leaflet is able to dynamically calculate the number of schools in a given region from just latitude and longitude data. You can experience this by zooming in and out of the graph. Here’s the code for it: lat&lt;-mean(schools$Latitude) lon&lt;-mean(schools$Longitude) leaflet(schools) %&gt;% addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&gt;% addWebGLHeatmap(size=15,units=&quot;px&quot;) %&gt;% setView(lng=lon,lat=lat, zoom = 10) 18.5 Dynamic Clustering Here we can see how leaflet allows one to dynamically cluster data based on its geographic distance at a given zoom level. Here’s the code for it: schools %&gt;% leaflet() %&gt;% addTiles() %&gt;% addCircleMarkers(radius = 2, label = ~htmlEscape(`School Name`), clusterOptions = markerClusterOptions()) 18.6 Plotting Groups Here’s the code for it: top&lt;- schools %&gt;% group_by(District)%&gt;% summarise(top=length(unique(`School Name`)),lon=mean(Longitude),lat=mean(Latitude))%&gt;% arrange(desc(top))%&gt;% head(10) pal &lt;- colorFactor(viridis(100),levels=top$District ) top %&gt;% leaflet(options = leafletOptions(dragging = TRUE)) %&gt;% addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %&gt;% addCircleMarkers(radius=~top/10,label=~paste0(&quot;District &quot;, District,&quot; - &quot;, top,&quot; Schools&quot;),color=~pal(District),opacity = 1) %&gt;% setView(lng=lon,lat=lat, zoom = 10) %&gt;% addLegend(&quot;topright&quot;, pal = pal, values = ~District, title = &quot;District&quot;, opacity = 0.8) 18.7 Plotting Categorical Data We can visualize the distribution of a particular class over the common map. This is achieved through an interactive widget provided on the top right that allows one to choose a particular category or multiple categories. The example below explores how schools in different neighborhoods are racially segregated. Here’s the code for it: ss &lt;- schools %&gt;% dplyr::select(`School Name`,Latitude, Longitude,`Percent White`, `Percent Black`, `Percent Asian`, `Percent Hispanic`) segregation &lt;- function(x){ majority = c() w &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent White`) b &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent Black`) a &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent Asian`) h &lt;- gsub(&quot;%&quot;,&quot;&quot;,x$`Percent Hispanic`) for (i in seq(1,nrow(ss))){ if (max(w[i],b[i],a[i],h[i]) == w[i]) {majority &lt;- c(majority,&#39;White&#39;)} else if (max(w[i],b[i],a[i],h[i]) == b[i]) {majority &lt;- c(majority,&#39;Black&#39;)} else if (max(w[i],b[i],a[i],h[i]) == a[i]) {majority &lt;- c(majority,&#39;Asian&#39;)} else if (max(w[i],b[i],a[i],h[i]) == h[i]) {majority &lt;- c(majority,&#39;Hispanic&#39;)} } return(majority) } ss$race &lt;- segregation(ss) white &lt;- ss %&gt;% filter(race == &quot;White&quot;) black &lt;- ss %&gt;% filter(race == &quot;Black&quot;) hispanic &lt;- ss %&gt;% filter(race == &quot;Hispanic&quot;) asian &lt;- ss %&gt;% filter(race ==&quot;Asian&quot;) lng &lt;- median(ss$Longitude) lat &lt;- median(ss$Latitude) pal_sector &lt;- colorFactor( viridis(4), levels = ss$race) m3 &lt;- leaflet() %&gt;% addProviderTiles(&quot;CartoDB&quot;) %&gt;% addCircleMarkers(data = white, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;White&quot;) m3 %&gt;% addCircleMarkers(data = black, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;Black&quot;) %&gt;% addCircleMarkers(data = hispanic, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;Hispanic&quot;) %&gt;% addCircleMarkers(data = asian, radius = 2, label = ~htmlEscape(`School Name`), color = ~pal_sector(race), group = &quot;Asian&quot;) %&gt;% addLayersControl(overlayGroups = c(&quot;White&quot;, &quot;Black&quot;,&quot;Hispanic&quot;,&quot;Asian&quot;)) %&gt;% setView(lng=lng,lat=lat,zoom=10) These examples provide only a glimpse to what is truly possible with this library. If you want to explore more features and use-cases, check out the links listed below. 18.8 External Resources Leaflet in R Documentation: main documentation of the package. Basic leaflet maps in R: tutorial with examples. Interesting Kaggle Kernel visualizing earthquake data using leaflet in R: another use-case to explore. "],
["network.html", "19 Interactive Networks 19.1 visNetwork (interactive)", " 19 Interactive Networks 19.1 visNetwork (interactive) visNetwork is a powerful R implementation of the interactive JavaScript vis.js library; it uses tidyverse piping: VisNetwork Docs. –&gt; The Vignette has clear worked-out examples: https://cran.r-project.org/web/packages/visNetwork/vignettes/Introduction-to-visNetwork.html The visNetwork documentation doesn’t provide the same level of explanation as the original, so it’s worth checking out the vis.js documentation as well: http://visjs.org/index.html In particular, the interactive examples are particularly useful for trying out different options. For example, you can test out physics options with this network configurator. 19.1.1 Minimum working example Create a node data frame with a minimum one of column (must be called id) with node names: # nodes boroughs &lt;- data.frame(id = c(&quot;The Bronx&quot;, &quot;Manhattan&quot;, &quot;Queens&quot;, &quot;Brooklyn&quot;, &quot;Staten Island&quot;)) Create a separate data frame of edges with from and to columns. # edges connections &lt;- data.frame(from = c(&quot;The Bronx&quot;, &quot;The Bronx&quot;, &quot;Queens&quot;, &quot;Queens&quot;, &quot;Manhattan&quot;, &quot;Brooklyn&quot;), to = c(&quot;Manhattan&quot;, &quot;Queens&quot;, &quot;Brooklyn&quot;, &quot;Manhattan&quot;, &quot;Brooklyn&quot;, &quot;Staten Island&quot;)) Draw the network with visNetwork(nodes, edges) library(visNetwork) visNetwork(boroughs, connections) Add labels by adding a label column to nodes: library(dplyr) boroughs &lt;- boroughs %&gt;% mutate(label = id) visNetwork(boroughs, connections) 19.1.2 Performance visNetwork can be very slow. %&gt;% visPhysics(stabilization = FALSE) starts rendering before the stabilization is complete, which does actually speed things up but allows you to see what’s happening, which makes a big difference in user experience. (It’s also fun to watch the network stabilize). Other performance tips are described here. 19.1.3 Helpful configuration tools %&gt;% visConfigure(enabled = TRUE) is a useful tool for configuring options interactively. Upon completion, click “generate options” for the code to reproduce the settings. More here (Note that changing options and then viewing them requires a lot of vertical scrolling in the browser. I’m not sure if anything can be done about this. If you have a solution, let me know!) 19.1.4 Coloring nodes Add a column of actual color names to the nodes data frame: boroughs &lt;- boroughs %&gt;% mutate(is.island = c(FALSE, TRUE, FALSE, FALSE, TRUE)) %&gt;% mutate(color = ifelse(is.island, &quot;blue&quot;, &quot;yellow&quot;)) visNetwork(boroughs, connections) 19.1.5 Directed nodes (arrows) visNetwork(boroughs, connections) %&gt;% visEdges(arrows = &quot;to;from&quot;, color = &quot;green&quot;) 19.1.6 Turn off the physics simulation It’s much faster without the simulation. The nodes are randomly placed and can be moved around without affecting the rest of the network, at least in the case of small networks. visNetwork(boroughs, connections) %&gt;% visEdges(physics = FALSE) 19.1.7 Grey out nodes far from selected (defined by “degree”) (Click a node to see effect.) # defaults to 1 degree visNetwork(boroughs, connections) %&gt;% visOptions(highlightNearest = TRUE) # set degree to 2 visNetwork(boroughs, connections) %&gt;% visOptions(highlightNearest = list(enabled = TRUE, degree = 2)) "],
["timeseriesbasic.html", "20 Time Series 20.1 Overview 20.2 Single/Multiple Time Series 20.3 Secular Trend 20.4 Seasonal Trends 20.5 Frequency of Data", " 20 Time Series This chapter originated as a community contribution created by HaiqingXu This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 20.1 Overview This section discusses drawing graphics for time series data. 20.2 Single/Multiple Time Series We can draw time series using geom_line() with time on the x-axis. X-axis should be an object in the Date class, assuming there is no hour/minute/second data. library(tidyverse) ggplot(data = economics, aes(x = date, y = pop))+ geom_line(color = &quot;blue&quot;) + ggtitle(&quot;US Population, in Thousands&quot;) + labs(x = &quot;year&quot;, y = &quot;population&quot;) We can also draw multiple time series on one plot for comparison purpose: df &lt;- read_csv(&quot;data/mortgage.csv&quot;) ## Parsed with column specification: ## cols( ## DATE = col_date(format = &quot;&quot;), ## `5/1 ARM` = col_double(), ## `15 YR FIXED` = col_double(), ## `30 YR FIXED` = col_double() ## ) df &lt;- df %&gt;% gather(key = TYPE, value = RATE, -DATE) %&gt;% mutate(TYPE = forcats::fct_reorder2(TYPE, DATE, RATE)) # puts legend in correct order g &lt;- ggplot(df, aes(DATE, RATE, color = TYPE)) + geom_line() + ggtitle(&quot;U.S. Mortgage Rates&quot;) + labs (x = &quot;&quot;, y = &quot;percent&quot;) + theme_grey(16) + theme(legend.title = element_blank()) g The following exmaple shows the closing price for four big technology companies in the US. When analyzing GDP, salary level and stock prices, it is often difficult to compare trends since the scales are so different. For example, since AAPL and MSFT prices per share are so much lower than GOOG’s price per share, it’s hard to discern the trends: library(tidyquant) stocks &lt;- c(&quot;AAPL&quot;, &quot;GOOG&quot;, &quot;IBM&quot;, &quot;MSFT&quot;) df &lt;- tq_get(stocks, from = as.Date(&quot;2013-01-01&quot;), to = as.Date(&quot;2013-12-31&quot;)) ggplot(df, aes(date, y = close, color = fct_reorder2(symbol, date, close))) + geom_line() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme(legend.title = element_blank()) In such a case, it can be helpful to rescale the data. We rescaled the data to make sure these four stocks have a price of 100 on Jan 2013: df &lt;- df %&gt;% group_by(symbol) %&gt;% mutate(rescaled_close = 100*close / close[1]) ggplot(df, aes(date, y = rescaled_close, color = fct_reorder2(symbol, date, rescaled_close))) + geom_line() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme(legend.title = element_blank()) 20.3 Secular Trend Instead of looking at observations over time, we often want to ovserve overall long-term trend in our time series data. In this case, we can use geom_smooth(). Here we will show secular trend using the Loess Smoother. AAPL &lt;- df %&gt;% filter(symbol == &quot;AAPL&quot;) g &lt;- ggplot(AAPL, aes(date, close)) + geom_point() g + geom_line(color = &quot;grey50&quot;) + geom_smooth(method = &quot;loess&quot;, se = FALSE, lwd = 1.5) + ggtitle(&quot;Loess Smoother for Apple Stock Price&quot;) + labs(x = &quot;Date&quot;, y = &quot;Price&quot;) Experiment with different smoothing parameters. g + geom_smooth(method = &quot;loess&quot;, span = .5, se = FALSE) 20.4 Seasonal Trends In addition to secular trends, there are also seasonal trends in time series data. One way is to visualize seasonal trends is to use fact on season(day of month, day of week etc.). library(lubridate) dfman &lt;- read_csv(&quot;data/ManchesterByTheSea.csv&quot;) ggplot(dfman, aes(Date, Gross)) + geom_line() + facet_grid(wday(Date, label = TRUE)~.) Or, let us create a monthly plot. monthplot(AirPassengers) 20.5 Frequency of Data What if you want to observe the frequency of time series data? A simple answer: use geom_point() in addition to geom_line(). # read file mydat &lt;- read_csv(&quot;data/WA_Sales_Products_2012-14.csv&quot;) %&gt;% mutate(Revenue = Revenue/1000000) # convert Quarter to a single numeric value Q mydat$Q &lt;- as.numeric(substr(mydat$Quarter, 2, 2)) # convert Q to end-of-quarter date mydat$Date &lt;- as.Date(paste0(mydat$Year, &quot;-&quot;, as.character(mydat$Q*3), &quot;-30&quot;)) Methoddata &lt;- mydat %&gt;% group_by(Date, `Order method type`) %&gt;% summarize(Revenue = sum(Revenue)) g &lt;- ggplot(Methoddata, aes(Date, Revenue, color = `Order method type`)) + geom_line(aes(group = `Order method type`)) + scale_x_date(limits = c(as.Date(&quot;2012-02-01&quot;), as.Date(&quot;2014-12-31&quot;)), date_breaks = &quot;6 months&quot;, date_labels = &quot;%b %Y&quot;) + ylab(&quot;Revenue in mil $&quot;) g + geom_point() There could be NA values in time series data. Using geom_point() with geom_line() is one way to detect missing values. Here we introduce another option: leave gaps. Methoddata$Date[year(Methoddata$Date)==2013] &lt;- NA g &lt;- ggplot(Methoddata, aes(Date, Revenue, color = `Order method type`)) + geom_path(aes(group = `Order method type`)) + scale_x_date(limits = c(as.Date(&quot;2012-02-01&quot;), as.Date(&quot;2014-12-31&quot;)), date_breaks = &quot;6 months&quot;, date_labels = &quot;%b %Y&quot;) + ylab(&quot;Revenue in mil $&quot;) g "],
["tidyquant.html", "21 Stock data with tidyquant 21.1 Overview 21.2 What is tidyquant? 21.3 Installing tidyquant 21.4 Single timeseries 21.5 Multiple timeseries 21.6 External Resources", " 21 Stock data with tidyquant This chapter originated as a community contribution created by naotominakawa This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 21.1 Overview This section covers how to use the tidyquant package to conduct timeseries analysis. 21.2 What is tidyquant? tidyquant is an one-stop shop for financial analysis. It is suitable for analyzing timeseries data, such as financial and economic data. tidyquant connects to various data sources such as Yahoo! Finance, Morning Star, Bloomberg market data, etc. It also behaves well with other Tidyverse packages. 21.3 Installing tidyquant You can install tidyquant from CRAN: install.packages(&quot;tidyquant&quot;) If you want to see which functions are available, you can run the following: # to see which functions are available (not run) library(tidyquant) tq_transmute_fun_options() 21.4 Single timeseries Obtain historical data for single stock (for example, Google): # get historical data for single stock. e.g. google library(tidyquant) tq_get(&quot;GOOGL&quot;, get=&quot;stock.prices&quot;) ## # A tibble: 2,708 x 7 ## date open high low close volume adjusted ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2009-01-02 154. 161. 153. 161. 7213700 161. ## 2 2009-01-05 161. 166. 158. 164. 9768200 164. ## 3 2009-01-06 167. 171. 163. 167. 12837500 167. ## 4 2009-01-07 164. 166. 160. 161. 8980000 161. ## 5 2009-01-08 159. 163. 159. 163. 7194100 163. ## 6 2009-01-09 164. 164. 157. 158. 8672300 158. ## 7 2009-01-12 158. 160. 155. 157. 6601900 157. ## 8 2009-01-13 156. 160. 155. 157. 8856100 157. ## 9 2009-01-14 155. 157. 149. 151. 10924800 151. ## 10 2009-01-15 149. 152. 144. 150. 11857100 150. ## # … with 2,698 more rows Calculate monthly return of single stock: library(dplyr) # calculate monthly return of single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) ## # A tibble: 130 x 2 ## date monthly_return ## &lt;date&gt; &lt;dbl&gt; ## 1 2009-01-30 0.0536 ## 2 2009-02-27 -0.00160 ## 3 2009-03-31 0.0298 ## 4 2009-04-30 0.138 ## 5 2009-05-29 0.0537 ## 6 2009-06-30 0.0104 ## 7 2009-07-31 0.0509 ## 8 2009-08-31 0.0420 ## 9 2009-09-30 0.0740 ## 10 2009-10-30 0.0812 ## # … with 120 more rows Create a line chart of the closing price for single stock: # showing closing price for single stock library(ggplot2) tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% ggplot(aes(date, close)) + geom_line() Create a line chart of the monthly return for single stock: # showing monthly return for single stock tq_get(c(&quot;GOOGL&quot;), get=&quot;stock.prices&quot;) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) %&gt;% ggplot(aes(date, monthly_return)) + geom_line() 21.5 Multiple timeseries Obtain historical data for multiple stocks (for example, GAFA): # get historical data for multiple stocks. e.g. GAFA tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) ## # A tibble: 9,981 x 8 ## symbol date open high low close volume adjusted ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GOOGL 2009-01-02 154. 161. 153. 161. 7213700 161. ## 2 GOOGL 2009-01-05 161. 166. 158. 164. 9768200 164. ## 3 GOOGL 2009-01-06 167. 171. 163. 167. 12837500 167. ## 4 GOOGL 2009-01-07 164. 166. 160. 161. 8980000 161. ## 5 GOOGL 2009-01-08 159. 163. 159. 163. 7194100 163. ## 6 GOOGL 2009-01-09 164. 164. 157. 158. 8672300 158. ## 7 GOOGL 2009-01-12 158. 160. 155. 157. 6601900 157. ## 8 GOOGL 2009-01-13 156. 160. 155. 157. 8856100 157. ## 9 GOOGL 2009-01-14 155. 157. 149. 151. 10924800 151. ## 10 GOOGL 2009-01-15 149. 152. 144. 150. 11857100 150. ## # … with 9,971 more rows Create a multiple line chart of the closing prices of multiple stocks (again, GAFA). We can show each stock in a different color on the same graph: # Create a multiple line chart of the closing prices of the four stocks, # showing each stock in a different color on the same graph. tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% ggplot(aes(date, close, color=symbol)) + geom_line() Transform the data so each stock begins at 100 and replot (Standardize the data so that we can compare timeseries): # Create a multiple line chart of the closing prices of the four stocks, # showing each stock in a different color on the same graph. # Transform the data so each stock begins at 100 and replot. tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% mutate(close = 100*close/first(close)) %&gt;% ggplot(aes(date, close, color=symbol)) + geom_line() Calculate monthly return of multiple stocks (again, GAFA): # calculate monthly return of multiple stocks tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) ## # A tibble: 480 x 3 ## # Groups: symbol [4] ## symbol date monthly_return ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 GOOGL 2009-01-30 0.0536 ## 2 GOOGL 2009-02-27 -0.00160 ## 3 GOOGL 2009-03-31 0.0298 ## 4 GOOGL 2009-04-30 0.138 ## 5 GOOGL 2009-05-29 0.0537 ## 6 GOOGL 2009-06-30 0.0104 ## 7 GOOGL 2009-07-31 0.0509 ## 8 GOOGL 2009-08-31 0.0420 ## 9 GOOGL 2009-09-30 0.0740 ## 10 GOOGL 2009-10-30 0.0812 ## # … with 470 more rows Create a multiple line chart of monthly return of the four stocks. Again, we can show each stock in a different color on the same graph: # Create a multiple line chart of monthly return of the four stocks, # showing each stock in a different color on the same graph tq_get(c(&quot;GOOGL&quot;,&quot;AMZN&quot;,&quot;FB&quot;,&quot;AAPL&quot;), get=&quot;stock.prices&quot;) %&gt;% group_by(symbol) %&gt;% tq_transmute(select=adjusted, mutate_fun=periodReturn, period=&quot;monthly&quot;, col_rename = &quot;monthly_return&quot;) %&gt;% ggplot(aes(date, monthly_return, color=symbol)) + geom_line() 21.6 External Resources tidyquant CRAN doc: formal documentation on the package tidyquant Github repo: Github repository for the tidyquant package with a great README "],
["import.html", "22 Importing Data 22.1 Overview 22.2 Import built-in dataset 22.3 Import local data 22.4 Import web data 22.5 Import data from database 22.6 More resources", " 22 Importing Data This chapter originated as a community contribution created by ZhangZhida This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 22.1 Overview This section covers how to import data from built-in R sources, local files, web sources and databases. 22.2 Import built-in dataset R comes with quite a lot of built-in datasets, which R users can play around with. You are probably familiar with many of the built-in datasets like iris, mtcars, beavers, dataset, etc. Since datasets are preloaded, we can manipulate them directly. To see a full list of built-in R datasets and their descriptions, please refer to The R Datasets Package. We can also run data() to view the full list. The most convenient option for viewing is ??datasets since provides a list of datasets in the Help pane. Clicking on a dataset will bring up its help file. There’s lots of important information about the sources of the data and the meaning of the variables in these help files, so be sure to check them out. Most datasets are lazy-loaded, which means that although they don’t appear as objects in the global environment, they are there when you reference them. However, for some packages, you must use data() to access the datasets, as follows: library(pgmm) data(wine) This is a common source of frustration for students: “I installed the library and loaded the package but the data’s not there!” Forewarned is forearmed. Packages that we use that fall in this category include: lawstat, pgmm, and others. (Submit a PR to add to this list.) 22.3 Import local data This section covers base R functions for reading data. For tidyverse versions (read_csv, read_delim, read_table, etc.) see the Data Import chapter of *R for Data Science.) 22.3.1 Import text file The function read.table() is the most general function for reading text files. To use this function, we need to specify how we read the file. In other words, we need to specify some basic parameters like sep, header, etc. sep represents the separator, and header is set to TRUE if we want to read the first line as the header information. Other parameters are also useful in different cases. For example, na.strings indicates strings should be regarded as NA values. df &lt;- read.table(&quot;data/MusicIcecream.csv&quot;, sep=&quot;,&quot;, header=TRUE) head(df) ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 22.3.2 Import CSV file A Comma-Separated Values file (CSV) is a delimited text file that uses a comma to separate values. We can easily read a CSV file with built-in R functions. The read.csv() function provides two useful parameters. One is header, which can be set to FALSE if there is no header. The other is sep, which specifies the separator. For example, we can specify the separator to be sep=&quot;\\t if the CSV file value is seperated by the tab character. The default value of header and sep are TRUE and &quot;,&quot;, respectively. read.csv2() is another function for reading CSV files. The difference between read.csv() and read.csv2 is that, the former uses the tab &quot;\\t&quot; as the separator, while the latter one uses the semicolon &quot;;&quot;. This serves as an easy shortcut for different CSV formats used in different regions. Let’s see an example on reading a standard CSV file: df &lt;- read.csv(&quot;data/MusicIcecream.csv&quot;) head(df) ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 A small note while reading multiple files: let R know your current directory by using setwd(). Then, you can read any file in this directory by directly using the name of the file, without specifying the location. 22.3.3 Import JSON file A JSON file is a file that stores simple data structures and objects in JavaScript Object Notation (JSON) format, which is a standard data interchange format. For example, {&quot;name&quot;:&quot;Vince&quot;, &quot;age&quot;:23, &quot;city&quot;:&quot;New York&quot;} is an object with JSON format. In recent years, JSON has become the mainstream format to transfer data on websites. To read a JSON file, we can use the jsonlite package. The jsonlite package is a JSON parser/generator optimized for the web. Its main strength is that it implements a bidirectional mapping between JSON data and the most important R data types. In the example below, the argument simplifyDataFrame = TRUE will directly transform a list of JSON objects into a dataframe. If you want to know more about the arguments simplifyVector and simplifyMatrix, which provide flexible control on other R data formats to transform to, please refer to Getting started with JSON and jsonlite. library(jsonlite) # read JSON data raw_json_data &lt;- fromJSON(txt = &quot;data/WaterConsumptionInNYC.json&quot;, simplifyDataFrame = TRUE) # transform JSON to Data Frame df &lt;- as.data.frame(raw_json_data) head(df) ## new_york_city_population nyc_consumption_million_gallons_per_day ## 1 7102100 1512 ## 2 7071639 1506 ## 3 7089241 1309 ## 4 7109105 1382 ## 5 7181224 1424 ## 6 7234514 1465 ## per_capita_gallons_per_person_per_day year ## 1 213 1979 ## 2 213 1980 ## 3 185 1981 ## 4 194 1982 ## 5 198 1983 ## 6 203 1984 22.4 Import web data 22.4.1 Read a data file directly into the workspace Let’s take the example of Water Consumption In The New York City, which is on the NYC Open Data website. We can import data from a URL just as we do with local data files. library(tidyverse) # specify the URL link to the data source url &lt;- &quot;https://data.cityofnewyork.us/api/views/ia2d-e54m/rows.csv&quot; # read the URL df &lt;- read_csv(url) head(df) ## # A tibble: 6 x 4 ## Year `New York City Pop… `NYC Consumption(Millio… `Per Capita(Gallons p… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1979 7102100 1512 213 ## 2 1980 7071639 1506 213 ## 3 1981 7089241 1309 185 ## 4 1982 7109105 1382 194 ## 5 1983 7181224 1424 198 ## 6 1984 7234514 1465 203 22.4.2 Scrape an HTML table using rvest Sometimes we wish to import data that appears as an HTML table on a web page. It might be a little messy, so best to first check if there’s another means for importing the data before moving forward. If not, rvest makes the process as painless as possible. Here’s a simple example. Suppose we wish to work with the borough data found on Wikipedia’s Boroughs of New York City page. First we read the page, find the tables, and then parse them with html_table: library(tidyverse) library(rvest) nyctables &lt;- read_html(&quot;https://en.wikipedia.org/wiki/Boroughs_of_New_York_City&quot;) %&gt;% html_nodes(&quot;table&quot;) %&gt;% html_table(fill = TRUE) nyctables is a list with three elements, one for each table on the page. Next we can check each list item until we find what we want, consulting the original web page to get a sense of where our table is located. (There are other methods for identifying what you need from a web page in more complex situations. See Additional Resources below.) It turns out that the table we want is the first list element: mytable &lt;- nyctables[[1]] head(mytable, 3) ## New York City&#39;s five boroughsvte New York City&#39;s five boroughsvte ## 1 Jurisdiction Jurisdiction ## 2 Borough County ## 3 The Bronx Bronx ## New York City&#39;s five boroughsvte New York City&#39;s five boroughsvte ## 1 Population Gross Domestic Product ## 2 Estimate (2017)[3] billions(US$)[4] ## 3 1,471,160 28.787 ## New York City&#39;s five boroughsvte New York City&#39;s five boroughsvte ## 1 Gross Domestic Product Land area ## 2 per capita(US$) square miles ## 3 19,570 42.10 ## New York City&#39;s five boroughsvte New York City&#39;s five boroughsvte ## 1 Land area Density ## 2 squarekm persons / sq. mi ## 3 109.04 34,653 ## New York City&#39;s five boroughsvte ## 1 Density ## 2 persons /sq. km ## 3 13,231 We can see that the column names are all the same due to the merged header in the original. We’ll fix the column names and remove the rows we don’t need: colnames(mytable) &lt;- c(&quot;borough&quot;, &quot;county&quot;, &quot;population&quot;, &quot;gdp_total&quot;, &quot;gdp_per_capita&quot;, &quot;land_sq_miles&quot;, &quot;land_sq_km&quot;, &quot;density_sq_miles&quot;, &quot;density_sq_km&quot;) # remove unneeded rows mytable &lt;- mytable %&gt;% slice(-c(1, 2, 10)) # convert character to numeric data where appropriate mytable &lt;- mytable %&gt;% mutate_at(vars(population:density_sq_km), parse_number) Now we’re good to go. Let’s draw a plot! options(scipen = 999) # turn off scientific notation mytable %&gt;% slice(1:5) %&gt;% select(borough, gdp_per_capita, land_sq_miles, population) %&gt;% gather(var, value, -borough) %&gt;% ggplot(aes(value, fct_reorder2(borough, var==&quot;gdp_per_capita&quot;, value, .desc = FALSE), color = borough)) + geom_point() + ylab(&quot;&quot;) + facet_wrap(~var, ncol = 1, scales = &quot;free_x&quot;) + guides(color = FALSE) Additional Resources Excellent webinar from RStudio on using rvest – covers how to use the structure of the HTML and CSS on the page to scrape the information that you need, as well as using additional rvest functions such as html_text(), html_name(), html_attrs(), html_children(), etc. 22.5 Import data from database R provides packages to manipulate data from relational databases like PostgreSQL, MySQL, etc. One of those packages is odbc package, which is one database interface for communication between R and relational database management systems. More resources on package: odbc. Before we connect to a local database, we must satisfy the requirement of the ODBC driver, through which our R package can communicate with the database. To get help on how to install ODBC driver on systems like Windows, Linux, MacOS, please refer to this document: Install ODBC Driver. After we installed the ODBC driver, with odbc and DBI packages, we are able to manipulate the database. To read a table in the database, we usually take steps as follows. First, we build the connection to the database using dbConnect() function. Then, we can do some exploratory operations like listing all tables in the database. To query the data we want, we can send a SQL query into the database. Then we can retrieve the desired data and dfFetch() provides control on how many records to retrieve at a time. Finally, we finish reading and close the connection. library(odbc) library(DBI) # build connection with database con &lt;- dbConnect(odbc::odbc(), driver = &quot;PostgreSQL Driver&quot;, database = &quot;test_db&quot;, uid = &quot;postgres&quot;, pwd = &quot;password&quot;, host = &quot;localhost&quot;, port = 5432) # list all tables in the test_db database dbListTables(con) # read table test_table into Data Frame data &lt;- dbReadTable(con, &quot;test_table&quot;) # write an R Data Frame object to an SQL table # here we write the built-in data mtcars to a new_table in DB data &lt;- dbWriteTable(con, &quot;new_table&quot;, mtcars) # SQL query result &lt;- dbSendQuery(con, &quot;SELECT * FROM test_table&quot;) # Retrieve the first 10 results first_10 &lt;- dbFetch(result, n = 10) # Retrieve the rest of the results rest &lt;- dbFetch(result) # close the connection dbDisconnect(con) 22.6 More resources Import local file: This R Data Import Tutorial Is Everything You Need Import JSON file: Getting started with JSON and jsonlite Import web data: The RCurl Package Import database file Databases using R Documentation on odbc package odbc Install ODBC Driver On Your System Install ODBC Driver "],
["tidy.html", "23 Walkthrough: Tidy Data &amp; dplyr 23.1 Overview 23.2 Installing packages 23.3 Viewing the data 23.4 What is Tidy data? 23.5 Tibbles 23.6 Test for missing values 23.7 Recode the missing values 23.8 Data wrangling verbs 23.9 Rename 23.10 Select 23.11 Mutate 23.12 Filter 23.13 Arrange 23.14 Summarize &amp; Group By 23.15 Pipe Operator 23.16 Tidying the transformed data 23.17 Helpful Links", " 23 Walkthrough: Tidy Data &amp; dplyr This chapter originated as a community contribution created by akshatapatel This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 23.1 Overview This example goes through some work with the biopsy dataset using dplyr functions to get to a tidy dataset. 23.1.1 Packages dplyr MASS tidyr 23.2 Installing packages Write the following statements in the console: install.packages('dplyr') install.packages('ggplot2') install.packages('tidyr') install.packages('MASS') Note: The first three packages are a part of the tidyverse, a collection of helpful packages in R, which can all be installed using install.packages('tidyverse'). dplyr is used for data wrangling and data transformation in data frames. The “d” in “dplyr” stands for “data frames” which is the most-used data type for storing datasets in R. 23.3 Viewing the data Let’s start with loading the package so we can get the data as a dataframe: #loading the dplyr library library(dplyr) #loading data from MASS:biopsy library(MASS) class(biopsy) ## [1] &quot;data.frame&quot; #glimpse is a part of the dplyr package glimpse(biopsy) ## Observations: 699 ## Variables: 11 ## $ ID &lt;chr&gt; &quot;1000025&quot;, &quot;1002945&quot;, &quot;1015425&quot;, &quot;1016277&quot;, &quot;1017023&quot;, &quot;10… ## $ V1 &lt;int&gt; 5, 5, 3, 6, 4, 8, 1, 2, 2, 4, 1, 2, 5, 1, 8, 7, 4, 4, 10, … ## $ V2 &lt;int&gt; 1, 4, 1, 8, 1, 10, 1, 1, 1, 2, 1, 1, 3, 1, 7, 4, 1, 1, 7, … ## $ V3 &lt;int&gt; 1, 4, 1, 8, 1, 10, 1, 2, 1, 1, 1, 1, 3, 1, 5, 6, 1, 1, 7, … ## $ V4 &lt;int&gt; 1, 5, 1, 1, 3, 8, 1, 1, 1, 1, 1, 1, 3, 1, 10, 4, 1, 1, 6, … ## $ V5 &lt;int&gt; 2, 7, 2, 3, 2, 7, 2, 2, 2, 2, 1, 2, 2, 2, 7, 6, 2, 2, 4, 2… ## $ V6 &lt;int&gt; 1, 10, 2, 4, 1, 10, 10, 1, 1, 1, 1, 1, 3, 3, 9, 1, 1, 1, 1… ## $ V7 &lt;int&gt; 3, 3, 3, 3, 3, 9, 3, 3, 1, 2, 3, 2, 4, 3, 5, 4, 2, 3, 4, 3… ## $ V8 &lt;int&gt; 1, 2, 1, 7, 1, 7, 1, 1, 1, 1, 1, 1, 4, 1, 5, 3, 1, 1, 1, 1… ## $ V9 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1… ## $ class &lt;fct&gt; benign, benign, benign, benign, benign, malignant, benign,… head(biopsy) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant 23.4 What is Tidy data? What does it mean for your data to be tidy? Tidy data has a standardized format and it is a consistent way to organize your data in R. Here’s the definition of Tidy Data given by Hadley Wickham: A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each observational unit forms a value in the table. See r4ds on tidy data for more info. What are the advantages of tidy data? Uniformity : It is easier to learn the tools that work with the data because they have a consistent way of storing data. Most built-in R functions work with vectors of values. Thus, having variables as columns/vectors allows R’s vectorized nature to shine. Can you observe and tell why this data is messy? The names of the columns such as V1, V2 are not intuitive in what they contain; good sign it is untidy. They are not different variables, but are values of a common variable. Now, we will see the how to transform our data using dplyr functions and then look at how to tidy our transformed data. 23.5 Tibbles A tibble is a modern re-imagining of the data frame. It is particularly useful for large datasets because it only prints the first few rows. It helps you confront problems early, leading to cleaner code. # Converting a df to a tibble biopsy &lt;- tbl_df(biopsy) biopsy ## # A tibble: 699 x 11 ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; ## 1 1000025 5 1 1 1 2 1 3 1 1 benign ## 2 1002945 5 4 4 5 7 10 3 2 1 benign ## 3 1015425 3 1 1 1 2 2 3 1 1 benign ## 4 1016277 6 8 8 1 3 4 3 7 1 benign ## 5 1017023 4 1 1 3 2 1 3 1 1 benign ## 6 1017122 8 10 10 8 7 10 9 7 1 malignant ## 7 1018099 1 1 1 1 2 10 3 1 1 benign ## 8 1018561 2 1 2 1 2 1 3 1 1 benign ## 9 1033078 2 1 1 1 2 1 1 1 5 benign ## 10 1033078 4 2 1 1 2 1 2 1 1 benign ## # … with 689 more rows 23.6 Test for missing values # Number of missing values in each column in the data frame colSums(is.na(biopsy)) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 0 0 0 0 0 0 16 0 0 0 0 The dataset contains missing values which need to be addressed. 23.7 Recode the missing values One way to deal with missing values is to recode them with the average of all the other values in that column: #change all the NAs to mean of the column biopsy$V6[is.na(biopsy$V6)] &lt;- mean(biopsy$V6, na.rm = TRUE) colSums(is.na(biopsy)) ## ID V1 V2 V3 V4 V5 V6 V7 V8 V9 class ## 0 0 0 0 0 0 0 0 0 0 0 See our chapter on time series with missing data for more info about dealing with missing data. 23.8 Data wrangling verbs Here are the most commonly used functions that help wrangle and summarize data: Rename Select Mutate Filter Arrange Summarize Group_by Select and mutate functions manipulate the variable (the columns of the data frame). Filter and arrange functions manipulate the observations (the rows of the data) ,whereas the summarize function manipulates groups of observations. All the dplyr functions work on a copy of the data and return a modified copy. They do not change the original data frame. If we want to access the results afterwards, we need to save the modified copy. 23.9 Rename The names of the columns in our biopsy data are very vague and do not give us the meaning of the values in that column. We need to change the names of the column so that the viewer gets a sense of the values they’re referring to. rename(biopsy, thickness = V1,cell_size = V2, cell_shape = V3, marg_adhesion = V4, epithelial_cell_size = V5, bare_nuclei = V6, chromatin = V7, norm_nucleoli = V8, mitoses = V9) ## # A tibble: 699 x 11 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## 6 1017… 8 10 10 8 7 ## 7 1018… 1 1 1 1 2 ## 8 1018… 2 1 2 1 2 ## 9 1033… 2 1 1 1 2 ## 10 1033… 4 2 1 1 2 ## # … with 689 more rows, and 5 more variables: bare_nuclei &lt;dbl&gt;, ## # chromatin &lt;int&gt;, norm_nucleoli &lt;int&gt;, mitoses &lt;int&gt;, class &lt;fct&gt; The tibble shown above is not saved and cannot be used further. To use it afterwards we save it as a new tibble: #saving the rename function output biopsy_new&lt;-rename(biopsy, thickness = V1,cell_size = V2, cell_shape = V3, marg_adhesion = V4, epithelial_cell_size = V5, bare_nuclei = V6, chromatin = V7, norm_nucleoli = V8, mitoses = V9) head(biopsy_new,5) ## # A tibble: 5 x 11 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## # … with 5 more variables: bare_nuclei &lt;dbl&gt;, chromatin &lt;int&gt;, ## # norm_nucleoli &lt;int&gt;, mitoses &lt;int&gt;, class &lt;fct&gt; The biopsy_new data frame can now be used for further manipulation. 23.10 Select Select returns a subset of the data. Specifically, only the columns that are specified are included. In the biopsy data, we do not require the variables “chromatin” and “mitoses”. So, let’s drop them using a minus sign: #selecting all except the columns chromatin and mitoses biopsy_new&lt;-select(biopsy_new,-chromatin,-mitoses) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 23.11 Mutate The mutate function computes new variables from the already existing variables and adds them to the dataset. It gives information that the data already contained but was never displayed. The “V6” variable contains the values of the bare nucleus from 1.00 to 10.00. If we wish to normalize the variable, we can use the mutate function: #normalize the bare nuclei values maximum_bare_nuclei&lt;-max(biopsy_new$bare_nuclei,na.rm=TRUE) biopsy_new&lt;-mutate(biopsy_new,bare_nuclei=bare_nuclei/maximum_bare_nuclei) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1000… 5 1 1 1 2 ## 2 1002… 5 4 4 5 7 ## 3 1015… 3 1 1 1 2 ## 4 1016… 6 8 8 1 3 ## 5 1017… 4 1 1 3 2 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 23.12 Filter Filter is the row-equivalent function of select; it returns a modified copy that contains only certain rows. This function filters rows based on the content and the conditions supplied in its argument. The filter function takes the data frame as the first argument. The next argument contains one or more logical tests. The rows/observations that pass these logical tests are returned in the result of the filter function. For our example, we only want the data of those tumor cells that have clump thickness greater than six as most of the malign tumors have this thickness looking at a plot of clump thickness vs tumor cell size grouped by class: library(ggplot2) ggplot(biopsy_new)+ geom_point(aes(x=thickness,y=cell_size,color=class))+ ggtitle(&quot;Plot of Clump Thickness Vs Tumor Cell Size&quot;) #normalize the bare nuclei values biopsy_new&lt;-filter(biopsy_new,thickness&gt;5.5) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1016… 6 8 8 1 3 ## 2 1017… 8 10 10 8 7 ## 3 1044… 8 7 5 10 7 ## 4 1047… 7 4 6 4 6 ## 5 1050… 10 7 7 6 4 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 23.13 Arrange Arrange reorders the rows of the data based on their contents in the ascending order by default. The doctors would want to view the data in the order of the cell size of the tumor. #arrange in the order of V2:cell size arrange(biopsy_new,cell_size) ## # A tibble: 186 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1050… 6 1 1 1 2 ## 2 1204… 6 1 1 1 2 ## 3 1223… 6 1 3 1 2 ## 4 5435… 6 1 3 1 4 ## 5 63375 9 1 2 6 4 ## 6 7529… 10 1 1 1 2 ## 7 1276… 6 1 1 3 2 ## 8 1238… 6 1 1 3 2 ## 9 1257… 6 1 1 1 1 ## 10 1224… 6 1 1 1 2 ## # … with 176 more rows, and 3 more variables: bare_nuclei &lt;dbl&gt;, ## # norm_nucleoli &lt;int&gt;, class &lt;fct&gt; This shows the data in increasing order of the cell size. To arrange the rows in decreasing order of V2, we add the desc() function to the variable before passing it to arrange. #arrange in the order of V2:cell size in decreasing order arrange(biopsy_new,desc(cell_size)) ## # A tibble: 186 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017… 8 10 10 8 7 ## 2 1080… 10 10 10 8 6 ## 3 1100… 6 10 10 2 8 ## 4 1103… 10 10 10 4 8 ## 5 1112… 8 10 10 1 3 ## 6 1116… 9 10 10 1 10 ## 7 1123… 6 10 2 8 10 ## 8 1168… 10 10 10 10 10 ## 9 1170… 10 10 10 8 2 ## 10 1173… 10 10 10 3 10 ## # … with 176 more rows, and 3 more variables: bare_nuclei &lt;dbl&gt;, ## # norm_nucleoli &lt;int&gt;, class &lt;fct&gt; As you can see, there are a number of rows with the same value of V2:cell_size. To break the tie, you can add another variable to be used for ordering when the first variable has the same value. Here, we use the tie breaker as the order of variable V3: by cell shape and by ID: #arrange in the order of V2:cell size biopsy_new&lt;-arrange(biopsy_new,desc(cell_size),desc(cell_shape),ID) head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017… 8 10 10 8 7 ## 2 1073… 10 10 10 10 6 ## 3 1080… 10 10 10 8 6 ## 4 1100… 6 10 10 2 8 ## 5 1100… 6 10 10 2 8 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; 23.14 Summarize &amp; Group By Summarize uses the data to create a new data frame with the summary statistics such as minimum, maximum, average, and so on. These statistical functions must be aggregate functions which take a vector of values as input and output a single value. The group_by function groups the data by the values of the variables. This, along with summarize, makes observations about groups of rows of the dataset. The doctors would want to see the maximum cell size and the thickness for each of the classes: benign and malignant. This can be done by grouping the data by class and finding the maximum of the required variables: biopsy_grouped &lt;- group_by(biopsy_new,class) summarize(biopsy_grouped, max(thickness), mean(cell_size), var(norm_nucleoli)) ## # A tibble: 2 x 4 ## class `max(thickness)` `mean(cell_size)` `var(norm_nucleoli)` ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 benign 8 2.67 5.93 ## 2 malignant 10 6.73 11.3 23.15 Pipe Operator What if we want to use the various data wrangling verbs together? This could be done by saving the result of each wrangling function in a new variable and using it for the next function as we did above. However, this is not recommended as: It requires extra typing and longer code. Unnecessary space is used up to save the various variables. If the data is large, this method slows down the analysis. The pipe operator can be used instead for the same purpose. The operator is placed between and object and the function. The pipe takes the object on its left and passes it as the first argument to the function to its right. The pipe operator is a part of the magrittr package. However, this package need not be loaded as the dplyr package makes life simpler and imports the pipe operator for us: biopsy_grouped &lt;- biopsy_new %&gt;% group_by(class) %&gt;% summarize(max(thickness),mean(cell_size),var(norm_nucleoli)) head(biopsy_grouped) ## # A tibble: 2 x 4 ## class `max(thickness)` `mean(cell_size)` `var(norm_nucleoli)` ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 benign 8 2.67 5.93 ## 2 malignant 10 6.73 11.3 23.16 Tidying the transformed data Have a look again at the messy data: # Messy Data head(biopsy_new,5) ## # A tibble: 5 x 9 ## ID thickness cell_size cell_shape marg_adhesion epithelial_cell… ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1017… 8 10 10 8 7 ## 2 1073… 10 10 10 10 6 ## 3 1080… 10 10 10 8 6 ## 4 1100… 6 10 10 2 8 ## 5 1100… 6 10 10 2 8 ## # … with 3 more variables: bare_nuclei &lt;dbl&gt;, norm_nucleoli &lt;int&gt;, ## # class &lt;fct&gt; Planning is required to decide which columns we need to keep unchanged, which ones to change, and what names are to be given to the new columns. The columns to keep are the ones that are already tidy. The ones to change are the ones that aren’t true variables but in fact levels of another variable. So, the ID and class columns are already tidy. These are kept as is. The columns V1:thickness, V2:cell_size, V3:cell_shape, V4:marg_adhesion, V5:epithelial_cell_size, V6:bare_nuclei, and V8:norm_nucleoli are not true variables but values of the variable Tumor_attributes. We can fix this with tidyr::gather(), which is used to convert data from messy to tidy. The gather function takes the data frame which we want to tidy as input. The next two parameters are the names of the key and the value columns in the tidy dataset. In our example, key=‘Tumor_Atrributes’ and value=‘Score’. You can also specify the columns that you do not want to be tidied, i.e. ID and class: #Tidy Data library(tidyr) tidy_df &lt;- biopsy_new %&gt;% gather(key = &quot;Tumor_Attributes&quot;, value = &quot;Score&quot;, -ID, -class) tidy_df ## # A tibble: 1,302 x 4 ## ID class Tumor_Attributes Score ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1017122 malignant thickness 8 ## 2 1073960 malignant thickness 10 ## 3 1080185 malignant thickness 10 ## 4 1100524 malignant thickness 6 ## 5 1100524 malignant thickness 6 ## 6 1103608 malignant thickness 10 ## 7 1112209 malignant thickness 8 ## 8 1116116 malignant thickness 9 ## 9 1116116 malignant thickness 9 ## 10 1168736 malignant thickness 10 ## # … with 1,292 more rows 23.17 Helpful Links r4ds on tidy data: It is always best to learn from the source, so a textbook written by Hadley Wickham is perfect. DataCamp dplyr course: This course covers the different fucntions in dplyr and how they manipulate data. "],
["missing.html", "24 Missing Data 24.1 Overview 24.2 tl;dr 24.3 What are NAs? 24.4 Types of Missing Data 24.5 Missing Patterns 24.6 Handling Missing values 24.7 External Resources", " 24 Missing Data This chapter originated as a community contribution created by ujjwal95 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 24.1 Overview This section covers what kinds of missing values are encountered in data and how to handle them. 24.2 tl;dr It’s difficult to handle missing data! If your data has some missing values, which it most likely will, you can either remove such rows, such columns, or impute them. 24.3 What are NAs? Whenever data in some row or column in your data is missing, it comes up as NA. Let’s have a look at some data, shall we? Name Sex Age E_mail Education Income Melissa Female 27 NA NA 1.0e+04 Peter NA NA peter.parker@esu.edu NA 7.5e+03 Aang Male 110 aang@avatars.com NA 1.0e+03 Drake Male NA NA NA 5.0e+04 Bruce NA 45 bruce.wayne@wayne.org NA 1.0e+07 Gwen Female 28 gwen.stacy@esu.edu NA 2.3e+04 Ash Male NA ash.ketchum@pokemon.com NA NA NA NA NA NA NA NA We can see the number of NAs in each column and row: colSums(is.na(data)) ## Name Sex Age E_mail Education Income ## 1 3 4 3 8 2 rowSums(is.na(data)) ## [1] 2 3 1 3 2 1 3 6 We can also see the ratio of the number of NAs in each column and row: colMeans(is.na(data)) ## Name Sex Age E_mail Education Income ## 0.125 0.375 0.500 0.375 1.000 0.250 rowMeans(is.na(data)) ## [1] 0.3333333 0.5000000 0.1666667 0.5000000 0.3333333 0.1666667 0.5000000 ## [8] 1.0000000 24.4 Types of Missing Data Missing Completely at Random (MCAR): These are missing data values which are not related to any missing or non-missing values in other columns in the data. Missing at Random (MAR): These are missing data which are linked to one or more groups in the data. The great thing about MAR is that MAR values can be predicted using other features. For example, it may be observed that people older than 70 generally do not enter their income. Most of the data we encounter is MAR. Missing Not at Random (MNAR): Generally, data which is not MAR is MNAR. A big problem is that there is not a huge distinction between MAR and MNAR. We generally assume MAR, unless otherwise known by an outside source. 24.5 Missing Patterns 24.5.1 Missing Patterns by columns We can see some missing patterns in data by columns, ggplot(tidy_names, aes(x = key, y = fct_rev(Name), fill = missing)) + geom_tile(color = &quot;white&quot;) + ggtitle(&quot;Names dataset with NAs added&quot;) + scale_fill_viridis_d() + theme_bw() And we can also add a scale to check the numerical values available in the dataset and look for any trends: library(scales) # for legend # Select columns having numeric values numeric_col_names &lt;- colnames(select_if(data, is.numeric)) filtered_for_numeric &lt;- tidy_names[tidy_names$key %in% numeric_col_names,] filtered_for_numeric$value &lt;- as.integer(filtered_for_numeric$value) # Use label=comma to remove scientific notation ggplot(data = filtered_for_numeric, aes(x = key, y = fct_rev(Name), fill = value)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient(low = &quot;grey80&quot;, high = &quot;red&quot;, na.value = &quot;black&quot;, label=comma) + theme_bw() Can you see the problem with the above graph? Notice that the scale is for all the variables, hence it cannot show the variable level differences! To solve this problem, we can standardize the variables: filtered_for_numeric &lt;- filtered_for_numeric %&gt;% group_by(key) %&gt;% mutate(Std = (value-mean(value, na.rm = TRUE))/sd(value, na.rm = TRUE)) %&gt;% ungroup() ggplot(filtered_for_numeric, aes(x = key, y = fct_rev(Name), fill = Std)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high =&quot;yellow&quot;, na.value = &quot;black&quot;) + theme_bw() Now, we can see the missing trends better! Let us sort them by the number missing by each row and column: # convert missing to numeric so it can be summed up filtered_for_numeric &lt;- filtered_for_numeric %&gt;% mutate(missing2 = ifelse(missing == &quot;yes&quot;, 1, 0)) ggplot(filtered_for_numeric, aes(x = fct_reorder(key, -missing2, sum), y = fct_reorder(Name, -missing2, sum), fill = Std)) + geom_tile(color = &quot;white&quot;) + scale_fill_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high =&quot;yellow&quot;, na.value = &quot;black&quot;) + theme_bw() 24.5.2 Missing Patterns by rows We can also see missing patterns in data by rows using the mi package: library(mi) x &lt;- missing_data.frame(data) ## Warning in .guess_type(y, favor_ordered, favor_positive, threshold, ## variable_name): Education : cannot infer variable type when all values are ## NA, guessing &#39;irrelevant&#39; ## NOTE: In the following pairs of variables, the missingness pattern of the second is a subset of the first. ## Please verify whether they are in fact logically distinct variables. ## [,1] [,2] ## [1,] &quot;Age&quot; &quot;Income&quot; ## [2,] &quot;Education&quot; &quot;Income&quot; ## Warning in .local(.Object, ...): Some observations are missing on all included variables. ## Often, this indicates a more complicated model is needed for this missingness mechanism image(x) Did you notice that the Education variable has been skipped? That is because the whole column is missing. Let us try to see some patterns in the missing data: x@patterns ## [1] E_mail Sex, Age ## [3] nothing Age, E_mail ## [5] Sex nothing ## [7] Age, Income Name, Sex, Age, E_mail, Income ## 7 Levels: nothing E_mail Sex Sex, Age Age, E_mail ... Name, Sex, Age, E_mail, Income levels(x@patterns) ## [1] &quot;nothing&quot; &quot;E_mail&quot; ## [3] &quot;Sex&quot; &quot;Sex, Age&quot; ## [5] &quot;Age, E_mail&quot; &quot;Age, Income&quot; ## [7] &quot;Name, Sex, Age, E_mail, Income&quot; summary(x@patterns) ## nothing E_mail ## 2 1 ## Sex Sex, Age ## 1 1 ## Age, E_mail Age, Income ## 1 1 ## Name, Sex, Age, E_mail, Income ## 1 We can visualize missing patterns using the visna (VISualize NA) function in the extracat package: extracat::visna(data) ## Warning in melt(as.data.frame(xs), ncol(xs)): The melt generic in ## data.table has been passed a data.frame and will attempt to redirect to the ## relevant reshape2 method; please note that reshape2 is deprecated, and this ## redirection is now deprecated as well. To continue using melt methods from ## reshape2 while both libraries are attached, e.g. melt.list, you can prepend ## the namespace like reshape2::melt(as.data.frame(xs)). In the next version, ## this warning will become an error. Here, the rows represent a missing pattern and the columns represent the column level missing values. The advantage of this graph is that it shows you only the missing patterns available in the data, not all the possible combinations of data (which will be 2^6 = 64), so that you can focus on the pattern in the data itself. We can sort the graph by most to least common missing pattern (i.e., by row): extracat::visna(data, sort = &quot;r&quot;) ## Warning in melt(as.data.frame(xs), ncol(xs)): The melt generic in ## data.table has been passed a data.frame and will attempt to redirect to the ## relevant reshape2 method; please note that reshape2 is deprecated, and this ## redirection is now deprecated as well. To continue using melt methods from ## reshape2 while both libraries are attached, e.g. melt.list, you can prepend ## the namespace like reshape2::melt(as.data.frame(xs)). In the next version, ## this warning will become an error. Or, by most to least missing values (i.e., by column): extracat::visna(data, sort = &quot;c&quot;) ## Warning in melt(as.data.frame(xs), ncol(xs)): The melt generic in ## data.table has been passed a data.frame and will attempt to redirect to the ## relevant reshape2 method; please note that reshape2 is deprecated, and this ## redirection is now deprecated as well. To continue using melt methods from ## reshape2 while both libraries are attached, e.g. melt.list, you can prepend ## the namespace like reshape2::melt(as.data.frame(xs)). In the next version, ## this warning will become an error. Or, by both row and column sort: extracat::visna(data, sort = &quot;b&quot;) ## Warning in melt(as.data.frame(xs), ncol(xs)): The melt generic in ## data.table has been passed a data.frame and will attempt to redirect to the ## relevant reshape2 method; please note that reshape2 is deprecated, and this ## redirection is now deprecated as well. To continue using melt methods from ## reshape2 while both libraries are attached, e.g. melt.list, you can prepend ## the namespace like reshape2::melt(as.data.frame(xs)). In the next version, ## this warning will become an error. 24.6 Handling Missing values There are multiple methods to deal with missing values. 24.6.1 Deletion of rows containing NAs Often we would delete rows that contain NAs when we are handling Missing Completely at Random data. We can delete the rows having NAs as below: na.omit(data) ## [1] Name Sex Age E_mail Education Income ## &lt;0 rows&gt; (or 0-length row.names) This method is called list-wise deletion. It removes all the rows having NAs. But we can see that the Education column is only NAs, so we can remove that column itself: edu_data &lt;- data[, !(colnames(data) %in% c(&quot;Education&quot;))] na.omit(edu_data) ## Name Sex Age E_mail Income ## 3 Aang Male 110 aang@avatars.com 1000 ## 6 Gwen Female 28 gwen.stacy@esu.edu 23000 Another method is pair-wise deletion, in which only the rows having missing values in the variable of interest are removed. 24.6.2 Imputation Techniques Imputation means to replace missing data with substituted values. These techniques are generally used with MAR data. 24.6.2.1 Mean/Median/Mode Imputation We can replace missing data in continuous variables with their mean/median and missing data in discrete/categorical variables with their mode. Either we can replace all the values in the missing variable directly, for example, if “Income” has a median of 15000, we can replace all the missing values in “Income” with 15000, in a technique known as Generalized Imputation. Or, we can replace all values on a similar case basis. For example, we notice that the income of people with Age &gt; 60 is much less than those with Age &lt; 60, on average, and hence we calculate the median income of each Age group separately, and impute values separately for each group. The problem with these methods is that they disturb the underlying distribution of the data. 24.6.3 Model Imputation There are several model based approaches for imputation of data, and several packages, like mice, Hmisc, and Amelia II, which deal with this. For more info, checkout this blog on DataScience+ about imputing missing data with the R mice package. 24.7 External Resources Missing Data Imputation - A PDF by the Stats Department at Columbia University regarding Missing-data Imputation How to deal with missing data in R - A 2 min read blogpost in missing data handling in R Imputing Missing Data in R; MICE package - A 9 min read on how to use the mice package to impute missing values in R How to Handle Missing Data - A great blogpost on how to handle missing data. "],
["outliers.html", "25 Outliers 25.1 Overview 25.2 tl;dr 25.3 What are outliers? 25.4 Types of Outliers 25.5 Handling Outliers 25.6 External Resources", " 25 Outliers This chapter originated as a community contribution created by kiransaini This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 25.1 Overview This section covers what types of outliers are encountered in data and how to handle them. 25.2 tl;dr I want to see my outliers! Outliers are difficult to spot because judging a datapoint as an outlier depends on the data or model with which they are compared. It is important to detect outliers because they can distort predictions and affect the accuracy of the model. 25.3 What are outliers? Outliers are noticeably far from the bulk of the data. They can be errors, genuine extreme values, rare values, unusual values, cases of special interest, or data from another source. Outliers on individual variables can be spotted using boxplots and bivariate outliers can be spotted using scatterplots. There can also be higher dimensional outliers that are not outliers in lower dimensions. It is worth identifying outliers for a number of reasons. Bad outliers should always be corrected and many statistical methods may work poorly in presence of outliers, but genuine outlying values can be interesting in their own right. Let’s have a look at the outliers of the ‘carat’ variable in the diamonds dataset: 25.4 Types of Outliers 25.4.1 Univariate Outliers Univariate outliers are outlying along one dimension. The best-known approach for an initial look at the data is to use boxplots. Tukey suggests marking individual cases as outliers if they are more than 1.5 IQR (the interquartile range) outside the hinges (basically the quartiles). Outliers may change if they are grouped by another variable. Let’s have a look at outliers on the Sepal Width variable in the iris dataset, both when the data is grouped by Species and when it is not. The outliers are clearly different: p &lt;- ggplot(iris, aes(x=Species, y=Sepal.Width)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Boxplot for Sepal Width grouped by Species in iris dataset&quot;) p p &lt;- ggplot(iris, aes(y=Sepal.Width)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Boxplot for Sepal Width in iris dataset&quot;) p 25.4.2 Multivariate Outliers Multivariate outliers are outlying along more than one dimension. Scatterplots and parallel coordinate plots are useful for visualizing multivariate outliers. You could regard points as outliers that are far from the mass of the data, or you could regard points as outliers that do not fit the smooth model well. Some points are outliers on both criteria. Let’s have a look at outliers on the Petal Length and Sepal Width variables in the iris dataset. We can clearly see an outlier which is far from the mass of the data (lower left): ggplot(iris, aes(x=Sepal.Width, y=Petal.Length)) + geom_point() + ggtitle(&quot;Scatterplot for Petal Length vs Sepal Width in iris dataset&quot;) Let’s have a look at outliers on the Petal Length and Petal Width variables in the iris dataset by fitting a smooth model. Here the outliers are the points that do not fit the smooth model: ggplot(iris, aes(x=Petal.Width, y=Petal.Length)) + geom_point() + geom_smooth() + geom_density2d(col=&quot;red&quot;,bins=4) + ggtitle(&quot;Scatterplot for Petal Length vs Petal Width in iris dataset&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Lets have a look at outliers in the diamond dataset using a parallel coordinate plot. We can see an outlier on the carat, cut, color, and clarity variables that is not an outlier on individual variables: library(GGally) ggparcoord(diamonds[1:1000,], columns=1:5, scale=&quot;uniminmax&quot;, alpha=0.8) + ggtitle(&quot;Parallel coordinate plot of diamonds dataset&quot;) 25.4.3 Categorical Outliers Outliers can be rare on a categorical scale. Certain combinations of categories are rare or should not occur at all. Fluctuation diagrams can be used to find such outliers. We can see rare cases in the HairEyeColor dataset: library(datasets) library(extracat) fluctile(HairEyeColor) 25.5 Handling Outliers Identifying outliers using plots and fitting models is relatively easy compared to what to do after identifying the outliers. Outliers can be rare cases, unusual values, or genuine errors. Genuine errors must be corrected if possible or else they must be removed. Imputation of outliers is complicated and appropriate background knowledge is required. A strategy for dealing with outliers is as follows Plot the one-dimensional distributions of the variables using boxplots. Examine any extreme outliers to see if they are rare values or errors and decide if they should be removed or imputed. For outliers which are extreme on one dimension, examine their values on other dimensions to decide whether they should be discarded or not. Discard values that are outliers on more than one dimension. Consider cases which are outliers in a higher dimensions but not in lower dimensions. Decide whether they are errors or not and consider discarding or imputing the errors. Plot boxplots and parallel coordinate plots by using grouping on a variable to find outliers in subsets of the data. 25.5.1 Not informative Consider the diamonds dataset. Let’s have a look at the width (y) and depth (z) variables: ggplot(diamonds, aes(y=y)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;#9B3535&quot;) + ggtitle(&quot;Ouliers on width variable in diamonds dataset&quot;) ggplot(diamonds, aes(y=z)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;#9B3535&quot;) + ggtitle(&quot;Ouliers on depth variable in diamonds dataset&quot;) ggplot(diamonds, aes(y, z)) + geom_point(col = &quot;#9B3535&quot;) + xlab(&quot;width&quot;) + ylab(&quot;depth&quot;) 25.5.2 More informative The plots are not very informative due to the outliers. The same plots after filtering the outliers are much more informative: d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y=y)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Ouliers on width variable in diamonds dataset&quot;) d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y=z)) + geom_boxplot(color=&quot;black&quot;, fill=&quot;lightblue&quot;) + ggtitle(&quot;Ouliers on depth variable in diamonds dataset&quot;) d2 &lt;- filter(diamonds, y &gt; 2 &amp; y &lt; 11 &amp; z &gt; 1 &amp; z &lt; 8) ggplot(d2, aes(y, z)) + geom_point(shape = 21, color = &quot;darkGrey&quot;, fill = &quot;lightBlue&quot;, stroke = 0.1) + xlab(&quot;width&quot;) + ylab(&quot;depth&quot;) 25.6 External Resources Identify, describe, plot, and remove the outliers from the dataset: Plotting and removing outliers from a dataset A Brief Overview of Outlier Detection Techniques: Discussion of the theoretical aspect of outlier detection "],
["dates.html", "26 Dates in R 26.1 Introduction 26.2 Converting to Date class 26.3 Working with Date Class 26.4 Plotting with a Date class variable", " 26 Dates in R 26.1 Introduction Working with dates and time can be very frustrating. In general, work with the least cumbersome class. That means if your variable is years, store it as an integer; there’s no reason to use a date or date-time class. If your variable does not involve time, use the Date class in R. 26.2 Converting to Date class You can convert character data to Date class with as.Date(): dchar &lt;- &quot;2018-10-12&quot; ddate &lt;- as.Date(dchar) Note that the two appear the same, although the class is different: dchar ## [1] &quot;2018-10-12&quot; ddate ## [1] &quot;2018-10-12&quot; class(dchar) ## [1] &quot;character&quot; class(ddate) ## [1] &quot;Date&quot; If the date is not in YYYY-MM-DD or YYYY/MM/DD form, you will need to specify the format to convert to Date class, using conversion specifications that begin with %, such as: as.Date(&quot;Thursday, January 6, 2005&quot;, format = &quot;%A, %B %d, %Y&quot;) ## [1] &quot;2005-01-06&quot; For a list of the conversion specifications available in R, see ?strptime. The tidyverse lubridate makes it easy to convert dates that are not in standard format with ymd(), ydm(), mdy(), myd(), dmy(), and dym() (among many other useful date-time functions): lubridate::mdy(&quot;April 13, 1907&quot;) ## [1] &quot;1907-04-13&quot; Try as.Date(&quot;April 13, 1907&quot;) and you will see the benefit of using a lubridate function. 26.3 Working with Date Class It is well worth the effort to convert to Date class, because there’s a lot you can do with dates in a Date class that you can’t do if you store the dates as character data. Number of days between dates: as.Date(&quot;2017-11-02&quot;) - as.Date(&quot;2017-01-01&quot;) ## Time difference of 305 days Compare dates: as.Date(&quot;2017-11-12&quot;) &gt; as.Date(&quot;2017-3-3&quot;) ## [1] TRUE Note that Sys.Date() returns today’s date as a Date class: Sys.Date() ## [1] &quot;2019-10-07&quot; class(Sys.Date()) ## [1] &quot;Date&quot; R has functions to pull particular pieces of information from a date: today &lt;- Sys.Date() weekdays(today) ## [1] &quot;Monday&quot; weekdays(today, abbreviate = TRUE) ## [1] &quot;Mon&quot; months(today) ## [1] &quot;October&quot; months(today, abbreviate = TRUE) ## [1] &quot;Oct&quot; quarters(today) ## [1] &quot;Q4&quot; The lubridate package provides additional functions to extract information from a date: today &lt;- Sys.Date() lubridate::year(today) ## [1] 2019 lubridate::yday(today) ## [1] 280 lubridate::month(today) ## [1] 10 lubridate::month(today, label = TRUE) ## [1] Oct ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec lubridate::mday(today) ## [1] 7 lubridate::week(today) ## [1] 40 lubridate::wday(today) ## [1] 2 26.4 Plotting with a Date class variable Both base R graphics and ggplot2 “know” how to work with a Date class variable, and label the axes properly: 26.4.1 base R df &lt;- read.csv(&quot;data/mortgage.csv&quot;) df$DATE &lt;- as.Date(df$DATE) plot(df$DATE, df$X5.1.ARM, type = &quot;l&quot;) # on the order of years plot(df$DATE[1:30], df$X5.1.ARM[1:30], type = &quot;l&quot;) # switch to months Note the the change in x-axis labels in the second graph. 26.4.2 ggplot2 # readr library(tidyverse) Note that unlike base Rread.csv(), readr::read_csv() automatically reads DATE in as a Date class since it’s in YYYY-MM-DD format: df &lt;- readr::read_csv(&quot;data/mortgage.csv&quot;) ## Parsed with column specification: ## cols( ## DATE = col_date(format = &quot;&quot;), ## `5/1 ARM` = col_double(), ## `15 YR FIXED` = col_double(), ## `30 YR FIXED` = col_double() ## ) g &lt;- ggplot(df, aes(DATE, `30 YR FIXED`)) + geom_line() + theme_grey(14) g ggplot(df %&gt;% filter(DATE &lt; as.Date(&quot;2006-01-01&quot;)), aes(DATE, `30 YR FIXED`)) + geom_line() + theme_grey(14) Again, when the data is filtered, the x-axis labels switch from years to months. 26.4.2.1 Breaks, limits, labels We can control the x-axis breaks, limits, and labels with scale_x_date(): library(lubridate) g + scale_x_date(limits = c(ymd(&quot;2008-01-01&quot;), ymd(&quot;2008-12-31&quot;))) + ggtitle(&quot;limits = c(ymd(\\&quot;2008-01-01\\&quot;), ymd(\\&quot;2008-12-31\\&quot;))&quot;) g + scale_x_date(date_breaks = &quot;4 years&quot;) + ggtitle(&quot;scale_x_date(date_breaks = \\&quot;4 years\\&quot;)&quot;) g + scale_x_date(date_labels = &quot;%Y-%m&quot;) + ggtitle(&quot;scale_x_date(date_labels = \\&quot;%Y-%m\\&quot;)&quot;) (Yes, even in the tidyverse we cannot completely escape the % conversion specification notation. Remember ?strptime for help.) 26.4.2.2 Annotations We can use geom_vline() with annotate() to mark specific events in a time series: ggplot(df, aes(DATE, `30 YR FIXED`)) + geom_line() + geom_vline(xintercept = ymd(&quot;2008-09-29&quot;), color = &quot;blue&quot;) + annotate(&quot;text&quot;, x = ymd(&quot;2008-09-29&quot;), y = 3.75, label = &quot; Market crash\\n 9/29/08&quot;, color = &quot;blue&quot;, hjust = 0) + scale_x_date(limits = c(ymd(&quot;2008-01-01&quot;), ymd(&quot;2009-12-31&quot;)), date_breaks = &quot;1 year&quot;, date_labels = &quot;%Y&quot;) + theme_grey(16) + ggtitle(&quot;`geom_vline()` with `annotate()`&quot;) "],
["percept.html", "27 Perception/Color Resources 27.1 Overview 27.2 Perception 27.3 Color 27.4 Quick tips on using color with ggplot2", " 27 Perception/Color Resources 27.1 Overview This section has resources for learning about graphical perception and how to use colors effectively. 27.2 Perception Here are some links to some key books/articles on perception: Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods: Classic article from William Cleveland and Robert McGill The Elements of Graphing Data: Textbook by William Cleveland Visualizing Data: Textbook by William Cleveland Creating More Effective Graphs: Textbook by Naomi Robbins 27.3 Color Color is very subjective. It is important to choose the right ones so that your work is easy to understand. Color Brewer: Excellent resource for getting effective color palettes for different projects. Its main focus is on cartography, but it is super useful for any project involving color. You can choose between different types of data (sequential, diverging, qualitative), ensure your chosen palette is effective for colorblind users (or print friendly or photocopy safe), and easily export the color palette to different formats (Adobe, GIMP/Inkscape, JS, CSS). The best go-to for effective color palettes. Color Blindness Simulator: Not sure how effective your project will be to a colorblind user? This tool can help. You can upload an image to see how it will look with different color vision handicaps. ColorPick Eyedropper: This Chrome extension allows you to copy hex color values from webpages. Simple and intuitive, it will make creating your awesome color palettes a lot easier. 27.4 Quick tips on using color with ggplot2 One of the most common problems is confusing color and fill. geom_point() and geom_line use color, many of the other geoms use fill. Some use both, such as geom_tile() in which case color is the border color and fill is the fill color. 27.4.1 Continuous data 27.4.1.1 ColorBrewer scale_color_distiller(palette = &quot;PuBu&quot;) or scale_fill_distiller(palette = &quot;PuBu&quot;) (What doesn’t work: scale_color_brewer(palette = &quot;PuBu&quot;)) 27.4.1.2 Viridis scale_color_viridis_c() or scale_fill... (the c stands for continuous) 27.4.1.3 Create your own + scale_color_gradient(low = &quot;white&quot;, high = &quot;red&quot;) or + scale_fill... + scale_color_gradient2(low = &quot;red&quot;, mid = &quot;white&quot;, high = &quot;blue&quot;, midpoint = 50) or + scale_fill... + scale_color_gradientn(colours = c(&quot;red&quot;, &quot;pink&quot;, &quot;lightblue&quot;, &quot;blue&quot;)) or scale_fill... 27.4.2 Discrete data 27.4.2.1 ColorBrewer scale_color_brewer(palette = &quot;PuBu&quot;) or scale_fill... 27.4.2.2 Viridis scale_color_viridis_d() or scale_fill... (the d stands for discrete) 27.4.2.3 Create your own + scale_color_manual(values = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) or scale_fill... + scale_fill_manual(values = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)) or scale_fill... "],
["themes.html", "28 Themes and Palettes 28.1 Overview 28.2 ggplot2 themes 28.3 RColorBrewer 28.4 ggthemes 28.5 ggthemr 28.6 ggsci 28.7 External Resources", " 28 Themes and Palettes This chapter originated as a community contribution created by ar3879 This page is a work in progress. We appreciate any input you may have. If you would like to help improve this page, consider contributing to our repo. 28.1 Overview Our graphs have to be informative and attractive to the audience to get their attention. Themes and colors play an important role in making the graphs attractive. This section covers how we can set different palettes and themes to suit the context and to make them look cool. 28.2 ggplot2 themes In ggplot2, we do have a set of themes, which we can set. A brief description of them is as follows: theme_gray(): signature ggplot2 theme theme_bw(): dark on light ggplot2 theme theme_linedraw(): uses black lines on white backgrounds only theme_light(): similar to linedraw() but with grey lines as well theme_dark(): lines on a dark background instead of light theme_minimal(): no background annotations, minimal feel theme_classic(): theme with no grid lines theme_void(): empty theme with no elements 28.2.1 ggplot2 theme example q &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5,alpha = 0.75) q + theme_minimal() There are several other packages available that set the themes and colors in many ways. We will discuss 4 of them. RColorBrewer ggthemes ggthemr ggsci 28.3 RColorBrewer Often, we find ourselves looking for the colors which make our graph look clear and cool. RColorBrewer offers a number of palettes, which we can use based on the context of our graph. There are three categories of these palettes: Sequential, Diverging, and Qualitative q &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) Sequential Palette: It represents the shade of the color from light to dark. It is usually used to represent interval data where low values can be shown with a light color and high values can be shown with a dark color. For instance –Blues, BuPu, YlGn, Reds, OrRd q + scale_colour_brewer(palette = &quot;Blues&quot;) Diverging Palette: It has darker colors of contrasting hues on both the ends, and lighter color in the middle. For instance –Spectral, RdGy, PuOr q + scale_colour_brewer(palette = &quot;PuOr&quot;) Qualitative Palette: It is usually used when we want to highlight the differences in the classes (categorial variables). For instance –set1, set2, set3, pastel1, pastel2 , dark2 q + scale_colour_brewer(palette = &quot;Pastel1&quot;) 28.4 ggthemes ggthemes provides additional geoms, scales, and themes to ggplot2. Some of them are really cool! We can change the theme and color of a graph based on the context. g1 &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5,alpha = 0.75) 28.4.1 ggthemes examples g1 + theme_economist() + scale_colour_economist() g1 + theme_igray() + scale_colour_tableau() g1 + theme_wsj() + scale_color_wsj() g1 + theme_igray() + scale_colour_colorblind() If we would like to use these colors in the graphs, which may not support using ggthemes, we can use the scales package to know what colors were used for a given palette. For example: show_col(colorblind_pal()(6)) 28.5 ggthemr ggthemr is used to set the theme of ggplot graphs. It has 17 different themes to change the way ggplot graphs look. Use of ggthemr is different from other other packages. We set the theme before using it. 28.5.1 ggthemr examples ggthemr(&quot;sky&quot;) ## Warning: New theme missing the following elements: axis.ticks.length.x, ## axis.ticks.length.x.top, axis.ticks.length.x.bottom, axis.ticks.length.y, ## axis.ticks.length.y.left, axis.ticks.length.y.right ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) ggthemr(&quot;flat&quot;) ## Warning: New theme missing the following elements: axis.ticks.length.x, ## axis.ticks.length.x.top, axis.ticks.length.x.bottom, axis.ticks.length.y, ## axis.ticks.length.y.left, axis.ticks.length.y.right ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) Interestingly, we can set more parameters to change the themes: ggthemr(&quot;lilac&quot;, type = &quot;outer&quot;, layout = &quot;scientific&quot;, spacing = 2) ## Warning: New theme missing the following elements: axis.ticks.length.x, ## axis.ticks.length.x.top, axis.ticks.length.x.bottom, axis.ticks.length.y, ## axis.ticks.length.y.left, axis.ticks.length.y.right ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) 28.6 ggsci ggsci offers a number of palettes inspired by colors used in scientific journals, science fiction movies, and TV shows. For continous data, scale_fill_material(colname) is used, and for discrete data, scale_color_palname() or scale_fill_palname() are used. 28.6.1 ggsci for discrete data # we need to remove the theme set previously if we don&#39;t want to use it anymore ggthemr_reset() g1 &lt;- ggplot(subset, aes(x = clarity, y = carat, color = cut)) + geom_point(size = 2.5, alpha = 0.75) g1 + scale_color_startrek() g1 + scale_color_jama() g1 + scale_color_locuszoom() 28.6.2 ggsci for continuous data ggplot(diamonds, aes(carat, price)) + geom_hex(bins = 20, color = &quot;red&quot;) + scale_fill_material(&quot;orange&quot;) We can also find out the color used, so that we can use them in some other graphs created in base R: palette = pal_lancet(&quot;lanonc&quot;, alpha = 0.7)(9) show_col(palette) 28.7 External Resources RColorBrewer: Setting up Color Palettes in R ggthemes: Github page containing more examples ggthemr: Github Repository of the package ggsci: Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2 "],
["github.html", "29 GitHub/Git Resources 29.1 Overview 29.2 First things first 29.3 The no branch workflow 29.4 Branching: your repo 29.5 Branching: someone else’s repo 29.6 Merging a pull request 29.7 Random git stuff that may be useful 29.8 Other resources", " 29 GitHub/Git Resources 29.1 Overview This section describes workflows for working with GitHub/Git and advice on how to collaborate in teams on large coding projects. If you’re interested in learning how to propose changes to edav.info (or any other repo) without leaving github.com, see our Contributing to this Resource chapter. Ok, not satisfied with fixing typos on GitHub? Ready to work locally and move code back and forth between your repositories (or someone else’s) and your machine? For that you’ll need Git, a widely used version control system. It is super useful and powerful, but people also find it quite annoying and difficult to understand. Rather than trying to master the whole system, we suggest beginning with some basic workflows, as outlined below. You can derive great benefits from it without being an expert (trust me, I know!) 29.2 First things first Install Git To do so, follow the instructions in the Install Git chapter of Happy Git with R. Tell Git your name and email address. Introduce yourself to Git in Happy Git explains it all. (Optional) Make sure that you can pull from and push to GitHub from your computer Connect to GitHub 29.3 The no branch workflow To get comfortable with Git, start with this basic workflow in which you will be pulling from and pushing to your repo on GitHub. Just you, no collaboration: The Connect RStudio to Git and GitHub chapter of Happy Git will get you set up: you will create a repo on GitHub, clone the repo into an RStudio project, and practice making changes. Once you’re set up, your local workflow will be pull, work, commit/push. PULL Each time you open RStudio and switch to the project, you will pull down any changes made to the repo on GitHub by clicking the Down Arrow in the Git pane of RStudio. You may think that no changes have been made to GitHub and there’s nothing to pull, but you may forget the typos that you fixed online, so it’s good practice to always start by pulling changes just in case. WORK Do your stuff. Make changes to files. Add new files. Keep an eye on the Git panel in RStudio; it will show you which files were changes. COMMIT/PUSH When you’re done working, you’ll want to think about what to do with the files that have changed. I like to keep the Git panel clear, so when I’m done I do one of three things with each file: 1) click “Staged” to get it ready for a sendoff to GitHub, 2) delete it if it’s not needed, 3) add it to .gitignore if it’s a file I want to keep locally but not send to GitHub. (Keep in mind that files in .gitignore are not backed up unless you have another backup system.) If I have a file that belongs somewhere else, I will move it there, so the only files left are the ones to send to GitHub. The next step is to click the Commit button and enter a commit message that meaningful describes what was done. Finally clicking the Up Arrow will send the commit to GitHub. It’s not considered good practice to commit too often, but as a beginner, it’s useful to do so to learn how it all works. 29.4 Branching: your repo Once you’ve comfortable with the workflow described above, you’re ready to start branching. The process is quite similar whether you’re working on your own repo or someone else’s. If it’s your repo, you can follow the steps in these slides, which provide step-by-step detail on creating a branch, doing work on the branch and then submitting a pull request to merge the changes into origin/master. Or you can follow the steps below, skipping steps 1 and 3. 29.5 Branching: someone else’s repo Step 1: Fork the upstream repo (once) Skip this step if you are syncing with your own repo. Let’s say you want to contribute to EDAV! Fork our GitHub repo and then on your own GitHub page, you will see a forked EDAV repo under the repositories section. Note, from now on, the term upstream repository refers to the original repo of the project that you forked and the term origin repository refers to the repo that you created or forked on GitHub. From your respective, both upstream and origin are remote repositories. A fork of jtr13/EDAV Step 2: Clone origin and create a local repository (once) A local repository is the repo that resides on your computer. In order to be able to work locally, we need to create a local copy of the remote reposiotry. Since we have already connected Git with RStudio, we can create a local repo in the following way. In RStudio, click File -&gt; New Project -&gt; Version Control -&gt; Git. Now you can fill in the url of the origin repo and click Create Project to create a local repository. Once created, make sure you can see a Terminal section in RStudio because this is where we type in all the Git command lines. Step 3: Configure remote that points to the upstream repository (once) Skip this step if you’re syncing with your own repo. The purpose of this step is to specify the location of the upstream repository. It may not make sense to you at this moment, but soon it will become clear. To complete this step, type in the following command line: &gt; git remote add upstream &lt;upstream repo url&gt; Source: Configuring a remote for a fork Step 4: Pull Under the Git tab in RStudio, you can see a pull button, which is represented by a downward pointing arrow. Clicking that button is equivalent to running a git pull command. The purpose of a pull command is to fetch and download content from a remote repository, and in this case we want to pull from the origin/master and make sure local/master is updated to match the content. Now we can create a new branch and work on new features that we want to add into the EDAV project. Notice that steps 1, 2 and 3 only need to be done once when you start working on a new project. However, whenever you want to create a new branch, pull command is always the first step as you want to keep your local repository up to date. Now you might have a question: what is a branch? Branching is the way to work on different parts of a repository at one time. It creates a snapshot of the original branch. Eventually, after you have finalized your work, you will merge your work into the upstream repository. To create a branch, Click the purple button under the Git tab, create a meaningful name for your branch. For example, if you hope to add more content into the histogram chapter in EDAV, name your branch “HistogramChapter”. In this way, both a local branch and a remote branch under the origin repo will be created. Step 5: Work, commit and push Be careful and make sure you are doing work under the local branch instead of the local master. Once you make some changes in your local repository (e.g. edit, add or delete a file), you will see the list of files that you have modified under the Git tab. This means, the changes you have made are in your working directory but not committed to local repository yet. You might feel confused about these terms, so let’s look at a graphical illustration. We have already explained the relationship between local repo and remote repo. In addition, there are two extra layers between the working directory (the place where you store all these files on your computer) and the local repo. To keep your local repo updated with the working directory, you need to first add the changes to a staging area and then commit these changes to the local repo. Finally, you want to push your local branch to the remote repo. When you use Git with RStudio, you do not need to write Git commands to add, commit or push. To add, you simply check the checkbox for each file you have modified. To commit, you just click on the commit button under the Git tab. In fact, you can commit as many times as needed. Finally, you can click on the push button, which is represented by an upward pointing arrow. Step 6: Submit a pull request Now you are able to see the branch you have created on the GitHub page. The next step is to submit a pull request and the process is very similar to the process described in the GitHub only walkthrough. Step 7: Delete the branch Once the merge is accepted, it’s good practice to delete this branch since the upstream already contains all the changes you have made. To delete the branch on origin, you can simply use GitHub. To delete the branch locally, type the following in the terminal: &gt; git branch -d branchname We also need to stop tracking the remote branch using the following Git command: &gt; git fetch -p. 29.5.1 Step 8: Sync your fork This should be done at the beginning of your next work session to make sure that the code you work on matches the version on GitHub. If it’s your repo, you can bring down the changes that were merged into origin/master with: &gt; git pull Note that the flow of new code is: local branch –&gt; origin/branch –&gt; origin/master –&gt; local master It’s important to remember to do this, because the next branch you create will be a copy of local master, and it needs to be up to date. If you’re working on someone else’s repo, make sure you’ve completed Step 3 above. Then, right before you start working again on the project, do the following to sync your fork: &gt; git fetch upstream &gt; git checkout master &gt; git merge upstream/master Source: Syncing a fork 29.6 Merging a pull request There are many methods to merge a request. The most direct simple and direct is to merge the PR on GitHub. This method works well for merging fixed typos and the like. If you want to be able to test code, you may want to check out the PR locally, test it, and perhaps even make edits to it before merging. Best practices in this area are evolving. My current recommendation is to use the usethis package, which makes complex tasks very simple. “How to edit a pull request locally” explains how to do so. Another great resource is “Explore and extend a pull request” in Happy Git with R. This chapter describes two official GitHub versions of merging a pull request, as well as a workflow in development using git2r. 29.7 Random git stuff that may be useful 29.7.1 Undo the last commit git reset --soft HEAD~1 29.7.2 Undo changes since the last commit git reset HEAD --hard 29.7.3 Forgot to branch (didn’t commit) Just create the new branch and changes will be moved there… changes in the working directory do not belong to a branch until they are committed (It will appear that the changes are on master too, but once you commit the changes, they will be gone from master.) 29.8 Other resources Getting Help If you’re lost, these might help. GitHub Guides: This is a phenomenal collection of short articles from GitHub to help you learn about the fundamentals around their product. They are so great, we have already listed their Hello World article. Here are some other important ones: Understanding the GitHub Flow: Explains how working with GitHub generally goes. Git Handbook: Explains what version control is. GitHub Help: This is the yellow-pages of GitHub. Ask a question and it will try to push you in the right direction. Get it? Branching out GitHub is super social. Learn how to git involved! Open Source Guide: Info on how to contribute to open source projects. Great links to the GitHub skills involved as well as good GitHub etiquette to adopt. Forking Projects: Quick read from GitHub on how to fork a repository so you can contribute to it. Mastering Issues: On what Issues are in GitHub and how they can help get things done. Our Page on Contributing: You can contribute to edav.info/ with your new-found GitHub skills! Checkout our page on how to contribute through pull requests and/or issues. More Resources To hit the ground running, checkout GitHub Learning Lab. This application will teach you how to use GitHub with hands-on courses using actual repos. Its the perfect way to understand what using GitHub looks like. For the nerds in the room… Git For Ages 4 And Up: There’s a lot going on under the hood. This talk will help explain how it all works…with kids toys! Make pretty git logs: Always remember (A DOG). Also, this alias command is nice to have around: git config --global alias.adog &quot;log --all --decorate --oneline --graph&quot; add and commit with one command: Another (even more) helpful alias command: git config --global alias.add-commit '!git add -A &amp;&amp; git commit' Git Aware Prompt: An excellent add-on to the Terminal that informs you which branch you have checked out. Someone also made an even spiffier version where it will inform you of your git status using helpful emojis. Contributing with git2r, on the Population Genetics in R provides helpful information on using git commands within R through the git2r package. In particular it explains how to create a GITHUB_PAT and then set the credential parameter in certain functions to find the PAT. (Note though that the site was created in 2015 and as of February 2019 has not been updated.) Want a little reading as well?: Resources to learn Git is a simple site split into two main sections: Learn by reading and Learn by doing. Take your pick. A Newbie’s Guide to Making A Pull Request (for an R package), Tony Elhabr’s experience submitting a pull request to an R package at tidyverse developer day (part of rstudio::conf 2019. "],
["publish.html", "30 Publishing Resources 30.1 Overview 30.2 tl;dr 30.3 Bookdown 30.4 Essentials 30.5 Adding a custom domain name 30.6 Make a custom 404 page 30.7 Hooking up Travis 30.8 Notes on our workflow 30.9 Other resources", " 30 Publishing Resources 30.1 Overview This section discusses how we built edav.info/ and includes references for building sites and books of your own using R. 30.2 tl;dr Want to get started making a site complete with Travis CI like this one? Zach Bogart has created a bookdown-template you can clone and build off of to create your own site. For instructions, consult the README file. 30.3 Bookdown edav.info/ is built using Bookdown, “a free and open-source R package built on top of R Markdown to make it really easy to write books and long-form articles/reports.” The biggest selling-point for bookdown is that it allows you to make content that is both professional and adaptable. If you want to update a regular book, you need to issue another edition and go through a lot of hassle. With bookdown, you can publish it in different formats (including print, if desired) and be able to change things easily when needed. We chose bookdown for edav.info/ because it allows us to present a lot of content in a compact, searchable manner, while also letting students suggest updates and contribute to its structure. Again, it is professional and adaptable (The default bookdown output is essentially just an online book, but we tried to liven it up by adding a lot of helpful icons, logos, and banners to improve navigation). Below are some helpful references we used in creating edav.info/, which may be helpful if you are interested in creating your own website or online resource with R. 30.4 Essentials How to Start a Bookdown Book: The hardest part about bookdown is getting it up and running. Sean Kross has the best template instructions we found. We started this project by cloning his template repo and building off of it. Excellent descriptions on what all the files do and what is essential to start your project. bookdown: Authoring Books and Technical Documents with R Markdown: This textbook by Yihui Xie, author of the bookdown package, explains everything bookdown is able to accomplish (published using bookdown…because of course it is). An incredible informative reference which we always kept close by. Author’s blurb: A guide to authoring books with R Markdown, including how to generate figures and tables, and insert cross-references, citations, HTML widgets, and Shiny apps in R Markdown. RStudio Bookdown Talk: Yihui Xie (author of the bookdown package) discusses his package and what it can do in a one-hour talk. Good for seeing finished examples. bookdown.org: Site for the bookdown package. Has a bunch of popular books published using bookdown and some info about how to get started using the package. Creating Websites in R: This tutorial, written by Emily Zabor (a Columbia alum), provides a thorough walkthrough for creating websites using different R tools. She discusses how to make different kinds of sites (personal, package, project, blog) as well as GitHub integration and step-by-step instructions for getting setup with templates and hosting. Very detailed and worth perusing if interested in making your own site. 30.5 Adding a custom domain name There are several parts to adding a custom domain name. Buy a domain name and edit DNS settings We used Google domains. In the registrar page, click the DNS icon and add the following to Custom resource records: NAME TYPE TTL DATA @ A 1h 185.199.108.153 www CNAME 1h @ Note that some tutorials list older IP addresses. Check here for the recommended ones. Change settings in your repo In Settings, add your custom domain name in the GitHub Pages section. Add a CNAME file to the gh-pages branch This is a very simple text file named CNAME (all caps). The contents should be one line with the custom domain name. For more detail on steps 2 and 3, see: Emily Zabor’s Tutorial on Custom Domains 30.6 Make a custom 404 page Your site may be lovely, but a default 404 page is always a let down. Not if but when someone types part of your URL incorrectly or a link gets broken, you should make sure there is something to see other than a boring backend page you had no input in designing. This article explains the process, but all you have to do is make a file called 404.html in your root directory and GitHub will use it rather than the default. Because of this, there is really no excuse for not having one. Here’s a look at our 404 page. Hopefully you aren’t seeing it that often. Some considerations: Always include a link back to the site: Throw the user a life-saver. Make it clear that something went wrong: Don’t hide the fact that this page is because of some error. Use absolute paths: The URL that throws the 404 error may be nested within unexpected folders. Make sure if you have any images or links, they work regardless of the file path (use “/images/…” rather than “images/…”, maybe link directly to css/homepage, etc.) Other than that, have fun with it!: There are plenty of examples of people making excellent 404 pages. It should make a frustrating experience just a little bit more bearable. 30.7 Hooking up Travis This tutorial is designed to help you add Travis to your GitHub Pages bookdown web site. It assumes you already have a working web site, with pages stored in a gh-pages branch. We’re not necessarily recommending the gh-pages route; we chose it since we found examples that worked for us using this method. Since the /docs folder is a newer and cleaner approach, it is certainly possible that it provides a better way to organize the repo. That said, there are various tutorials for how to set up the gh-pages branch; it appears that the best way to do so is to create an orphan branch, as explained here. We should note that this makes it all seem very easy to add Travis, which actually was not the case at all for us. I guess everything looks easy in retrospect. If you run into trouble, let us know by filing an issue or submitting a pull request. More info on all the contribution stuff can be found on our contribute page. 30.7.1 Add Travis files to GitHub repo Add these files to your repo: https://github.com/rstudio/bookdown-demo/blob/master/.travis.yml No changes https://github.com/rstudio/bookdown-demo/blob/master/_build.sh Remove the last two lines if you’re only interested in a GitHub Pages book. https://github.com/rstudio/bookdown-demo/blob/master/_deploy.sh The only changes you need to make are to the git config lines. You need to use your GitHub email, but the username can be anything. 30.7.2 Add Travis service Create a Travis account on www.travis-ci.org by clicking on “Sign in with GitHub” on the top right. Click Authorize to allow Travis to have proper access to GitHub. Go back to GitHub and create a personal access token (PAT) if you don’t have one already. You can do so here. Note that you must save your PAT somewhere because you can’t access it once it’s created. Also note that the PAT provides a means to access your GitHub repo through an API, an alternative means to logging in with your username/password (There is an API Token in Travis but this is not the one to use). Return to your Travis profile (travis-ci.org/profile/[GITHUB username]) and click the button next to the appropriate repo to toggle it on. Click on Settings next to the button and add your saved GITHUB_PAT under Environmental Variables: set “Name” to “GITHUB_PAT” and “Value” to the value of the token. If all goes well, you can sit back, relax, and watch Travis do the work for you. via GIPHY 30.8 Notes on our workflow 30.8.1 30.9 Other resources blogdown: Creating Websites with R Markdown: Textbook on the blogdownpackage, another option for generating websites with R. Getting Started with GitHub Pages: Short article from GitHub Guides on creating/hosting a website using GitHub Pages. A Beginner’s Guide to Travis CI for R: Fantastic blog post by Julia Silge, includes debugging advice that helped us solve a problem involving installing packages with system requirements. "],
["workflow.html", "31 Workflow Notes", " 31 Workflow Notes The purpose of this chapter is to provide information to research assistants, teaching assistants etc. on maintaining this website. Get familiar with the bookdown package. Start with a minimal bookdown book. Understand the workflow of contributing to EDAV using Git. Before you work on the EDAV repo, practice using the bookdown-practice repository. Feel free to submit pull requests to the upstream repo for practice purpose. "],
["general.html", "32 General Resources 32.1 Books 32.2 Cheatsheets 32.3 Articles 32.4 Meetups 32.5 Twitter 32.6 Data", " 32 General Resources This is a long list of helpful general resources related to EDAV. If you have come across a good resource you don’t see here, consider adding it with a pull request (see the contribute page for more info). 32.1 Books A lot of these are available for students through Columbia Libraries, in both physical and e-book formats. Graphical Data Analysis with R: This book systematically goes through the different types of data, including categorical variables, continuous variables, and time series. The author shows different examples of plotting techniques using ggplot and promoting the “grammar of graphics” model. Code snippets included and available at the book’s website. R for Data Science: The classic. Everything from data types, programming, modeling, communicating, and those keyboard shortcuts you keep forgetting. To quote the book, “this book will teach you how to do data science with R.” Nuff said. 32.2 Cheatsheets Cheatsheet of cheatsheets: Paul van der Laken has put together a large collection of R resource links, including cheat sheets, style guides, package info, blogs, and other helpful resources. RStudio Cheatsheet Collection: Collection of downloadable cheatsheets from RStudio. Includes ones on R Markdown, Data Transformation (dplyr), and Data Visualization (ggplot2). They also have a R Markdown Reference Guide, which is great for remembering that one chunk option that’s on the tip of your tongue. R Base Graphics Cheatsheet: Oddly enough, despite the length of time it’s been around, it’s hard to find a base graphics cheatsheet. Joyce put this one together to help you out if you’re using base graphics. 32.3 Articles Ten Simple Rules for Better Figures: A helpful article discussing how to make the best figures possible by following ten basic rules such as “Avoid ‘chartjunk’” and “Know Your Audience”. Good to keep these rules in mind. The Simpsons by the Data: Nice example of telling a story with data (histograms, scatterplots, etc.). Also, it’s subject is everybody’s favorite TV family. 32.4 Meetups New York Open Statistical Programming Meetup: Meetups hosted by Jared Lander and Wes McKinney on a variety of topics in statistical programming, but with a focus on the R language. Past speakers have included J.J. Allaire (founder of RStudio) and Hadley Wickham (core tidyverse developer). Other attendees are generally eager to welcome newcomers and all of their talks are available on the Lander Analytics Youtube channel. 32.5 Twitter R likes Twitter. Here are some cool people doing work with #rstats: Hadley Wickham David Robinson Julia Silge 32.6 Data (Perhaps this should be a new chapter) Members of the United States Congress (1789-Present) with lots of biographical information https://github.com/unitedstates/congress-legislators "],
["chapter-index.html", "33 Chapter Index by Resource Type 33.1 Overview 33.2 Index", " 33 Chapter Index by Resource Type 33.1 Overview This page includes links to every chapter in edav.info/ Click on a banner to go to the desired page. If you’re wondering, here’s an explanation of what the banner colors mean. 33.2 Index "]
]
